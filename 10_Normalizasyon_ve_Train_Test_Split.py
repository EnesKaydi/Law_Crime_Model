"""
10_Normalizasyon_ve_Train_Test_Split.py

Bu script:
- Ä°ÅŸlenmiÅŸ veriyi (`wcld_Processed_For_Model.csv`) yÃ¼kler
- Hedef deÄŸiÅŸkenleri (jail, release) ayÄ±rÄ±r
- Feature'larÄ± StandardScaler ile normalize eder
- Stratified train-test split yapar (80-20)
- Ceza kategorilerine gÃ¶re stratification (class imbalance iÃ§in)
- Train ve test setlerini kaydeder
- Scaler objesini kaydeder (.pkl formatÄ±nda - model deployment iÃ§in)
- TÃ¼m adÄ±mlarÄ± SONUCLAR.md'ye kaydeder

KullanÄ±m:
    /Users/muhammedeneskaydi/PycharmProjects/LAW/.venv/bin/python 10_Normalizasyon_ve_Train_Test_Split.py

Notlar:
- StandardScaler: mean=0, std=1 yapacak (XGBoost iÃ§in iyi)
- Stratified split: Ceza kategorileri dengelenmesi iÃ§in (Hafif/Orta/AÄŸÄ±r)
- random_state=42: Tekrarlanabilirlik
- Scaler deployment iÃ§in gerekli (production'da aynÄ± normalizasyon uygulanmalÄ±)
"""

import os
import pickle
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# --- Ayarlar ---
BASE_DIR = "/Users/muhammedeneskaydi/PycharmProjects/LAW"
PROCESSED_CSV = os.path.join(BASE_DIR, "wcld_Processed_For_Model.csv")
OUTPUT_DIR = os.path.join(BASE_DIR, "model_data")
SONUCLAR_PATH = os.path.join(BASE_DIR, "SONUCLAR.md")

os.makedirs(OUTPUT_DIR, exist_ok=True)

print("=" * 70)
print("ADIM 7: NORMALIZASYON VE TRAIN-TEST SPLIT")
print("=" * 70)

# --- Veri YÃ¼kleme ---
print(f"\nğŸ“‚ Ä°ÅŸlenmiÅŸ veri yÃ¼kleniyor: {PROCESSED_CSV}")
df = pd.read_csv(PROCESSED_CSV)
print(f"âœ… Veri yÃ¼klendi. SatÄ±r: {len(df):,}, Kolon: {len(df.columns)}")

# ===== 1. HEDEF DEÄÄ°ÅKENLERÄ° AYIR =====
print("\n" + "=" * 70)
print("1. HEDEF DEÄÄ°ÅKENLERÄ° AYIRMA")
print("=" * 70)

# Hedef deÄŸiÅŸkenler
target_vars = ['jail', 'release']
print(f"\n  ğŸ¯ Hedef deÄŸiÅŸkenler: {target_vars}")

# Jail deÄŸeri olmayan kayÄ±tlarÄ± Ã§Ä±kar (NaN veya 0)
print(f"  ğŸ” Jail deÄŸeri kontrol ediliyor...")
original_len = len(df)
df_valid = df[df['jail'].notna() & (df['jail'] > 0)].copy()
removed_len = original_len - len(df_valid)

print(f"  âœ… Jail deÄŸeri olan kayÄ±tlar seÃ§ildi")
print(f"    â€¢ Orijinal: {original_len:,}")
print(f"    â€¢ GeÃ§erli: {len(df_valid):,}")
print(f"    â€¢ Ã‡Ä±karÄ±lan: {removed_len:,} (%{removed_len/original_len*100:.2f})")

# Hedef deÄŸiÅŸkenleri ayÄ±r
y = df_valid[target_vars].copy()
X = df_valid.drop(columns=target_vars)

print(f"\n  ğŸ“Š X (Features): {X.shape}")
print(f"  ğŸ“Š y (Targets): {y.shape}")

# ===== 2. CEZA KATEGORÄ°LERÄ° OLUÅTURMA (STRATÄ°FÄ°CATÄ°ON Ä°Ã‡Ä°N) =====
print("\n" + "=" * 70)
print("2. CEZA KATEGORÄ°LERÄ° OLUÅTURMA (STRATIFICATION)")
print("=" * 70)

# Jail deÄŸerlerine gÃ¶re kategoriler (EDA'da kullandÄ±ÄŸÄ±mÄ±z gibi)
def categorize_jail(val):
    if val <= 180:
        return 'Hafif'
    elif val <= 1080:
        return 'Orta'
    else:
        return 'Agir'

y['jail_category'] = y['jail'].apply(categorize_jail)

category_counts = y['jail_category'].value_counts()
print(f"\n  ğŸ“Š Ceza Kategorileri DaÄŸÄ±lÄ±mÄ±:")
for cat, count in category_counts.items():
    pct = count / len(y) * 100
    print(f"    â€¢ {cat}: {count:,} (%{pct:.2f})")

# ===== 3. FEATURE NORMALÄ°ZASYONU =====
print("\n" + "=" * 70)
print("3. FEATURE NORMALÄ°ZASYONU (STANDARDSCALER)")
print("=" * 70)

print(f"\n  âš™ï¸ StandardScaler uygulanÄ±yor...")
print(f"    â€¢ TÃ¼m feature'lar mean=0, std=1 yapÄ±lacak")

scaler = StandardScaler()

# Sadece sayÄ±sal kolonlarÄ± normalize et (zaten hepsi sayÄ±sal olmalÄ±)
numeric_cols = X.select_dtypes(include=[np.number]).columns
print(f"    â€¢ Normalize edilecek kolon: {len(numeric_cols)}")

# Fit ve transform
X_scaled = scaler.fit_transform(X[numeric_cols])
X_scaled_df = pd.DataFrame(X_scaled, columns=numeric_cols, index=X.index)

print(f"  âœ… Normalizasyon tamamlandÄ±")
print(f"    â€¢ Ã–rnek Ã¶ncesi deÄŸerler: {X[numeric_cols[0]].head(3).values}")
print(f"    â€¢ Ã–rnek sonrasÄ± deÄŸerler: {X_scaled_df[numeric_cols[0]].head(3).values}")

# ===== 4. TRAIN-TEST SPLIT =====
print("\n" + "=" * 70)
print("4. TRAIN-TEST SPLIT (STRATIFIED)")
print("=" * 70)

print(f"\n  ğŸ”€ Stratified split uygulanÄ±yor...")
print(f"    â€¢ Train: %80")
print(f"    â€¢ Test: %20")
print(f"    â€¢ Stratify: jail_category (Hafif/Orta/AÄŸÄ±r)")
print(f"    â€¢ Random state: 42")

# Split yap
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_df,
    y,
    test_size=0.20,
    random_state=42,
    stratify=y['jail_category']
)

print(f"\n  âœ… Split tamamlandÄ±!")
print(f"    â€¢ X_train: {X_train.shape}")
print(f"    â€¢ X_test: {X_test.shape}")
print(f"    â€¢ y_train: {y_train.shape}")
print(f"    â€¢ y_test: {y_test.shape}")

# Kategori daÄŸÄ±lÄ±mlarÄ± kontrol et
print(f"\n  ğŸ“Š Train set kategori daÄŸÄ±lÄ±mÄ±:")
train_cats = y_train['jail_category'].value_counts()
for cat, count in train_cats.items():
    pct = count / len(y_train) * 100
    print(f"    â€¢ {cat}: {count:,} (%{pct:.2f})")

print(f"\n  ğŸ“Š Test set kategori daÄŸÄ±lÄ±mÄ±:")
test_cats = y_test['jail_category'].value_counts()
for cat, count in test_cats.items():
    pct = count / len(y_test) * 100
    print(f"    â€¢ {cat}: {count:,} (%{pct:.2f})")

# ===== 5. VERÄ°LERÄ° KAYDET =====
print("\n" + "=" * 70)
print("5. VERÄ°LERÄ° KAYDETME")
print("=" * 70)

print(f"\n  ğŸ’¾ Train ve test setleri kaydediliyor...")

# Train set
X_train.to_csv(os.path.join(OUTPUT_DIR, 'X_train.csv'), index=False)
y_train.to_csv(os.path.join(OUTPUT_DIR, 'y_train.csv'), index=False)

# Test set
X_test.to_csv(os.path.join(OUTPUT_DIR, 'X_test.csv'), index=False)
y_test.to_csv(os.path.join(OUTPUT_DIR, 'y_test.csv'), index=False)

print(f"  âœ… CSV dosyalarÄ± kaydedildi:")
print(f"    â€¢ {OUTPUT_DIR}/X_train.csv")
print(f"    â€¢ {OUTPUT_DIR}/y_train.csv")
print(f"    â€¢ {OUTPUT_DIR}/X_test.csv")
print(f"    â€¢ {OUTPUT_DIR}/y_test.csv")

# ===== 6. SCALER OBJESÄ°NÄ° KAYDET =====
print("\n" + "=" * 70)
print("6. SCALER OBJESÄ°NÄ° KAYDETME")
print("=" * 70)

scaler_path = os.path.join(OUTPUT_DIR, 'scaler.pkl')
print(f"\n  ğŸ’¾ Scaler objesi kaydediliyor: {scaler_path}")

with open(scaler_path, 'wb') as f:
    pickle.dump(scaler, f)

print(f"  âœ… Scaler kaydedildi!")
print(f"    â€¢ Deployment'ta aynÄ± scaler kullanÄ±lacak")
print(f"    â€¢ Yeni veri gelince: scaler.transform(new_data)")

# ===== 7. FEATURE NAMES KAYDET =====
print("\n" + "=" * 70)
print("7. FEATURE Ä°SÄ°MLERÄ°NÄ° KAYDETME")
print("=" * 70)

feature_names_path = os.path.join(OUTPUT_DIR, 'feature_names.txt')
print(f"\n  ğŸ’¾ Feature isimleri kaydediliyor: {feature_names_path}")

with open(feature_names_path, 'w') as f:
    for col in X_train.columns:
        f.write(f"{col}\n")

print(f"  âœ… {len(X_train.columns)} feature ismi kaydedildi")

# ===== 8. Ã–ZET Ä°STATÄ°STÄ°KLER =====
print("\n" + "=" * 70)
print("8. Ã–ZET Ä°STATÄ°STÄ°KLER")
print("=" * 70)

print(f"\n  ğŸ“Š Final Veri Seti Ã–zeti:")
print(f"    â€¢ Toplam veri: {len(df_valid):,} satÄ±r")
print(f"    â€¢ Feature sayÄ±sÄ±: {X_train.shape[1]}")
print(f"    â€¢ Hedef deÄŸiÅŸken: 2 (jail, release)")
print(f"    â€¢ Train set: {len(X_train):,} (%80)")
print(f"    â€¢ Test set: {len(X_test):,} (%20)")
print(f"    â€¢ Normalizasyon: StandardScaler (mean=0, std=1)")
print(f"    â€¢ Stratification: jail_category (Hafif/Orta/AÄŸÄ±r)")

# Hedef deÄŸiÅŸken istatistikleri
print(f"\n  ğŸ“Š Hedef DeÄŸiÅŸken Ä°statistikleri (Train):")
print(f"    â€¢ jail ortalama: {y_train['jail'].mean():.2f} gÃ¼n")
print(f"    â€¢ jail median: {y_train['jail'].median():.2f} gÃ¼n")
print(f"    â€¢ jail std: {y_train['jail'].std():.2f} gÃ¼n")
print(f"    â€¢ jail min: {y_train['jail'].min():.0f} gÃ¼n")
print(f"    â€¢ jail max: {y_train['jail'].max():.0f} gÃ¼n")

# ===== 9. SONUCLAR.MD'YE EKLEME =====
print("\n" + "=" * 70)
print("9. SONUCLAR.MD GÃœNCELLEME")
print("=" * 70)

now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

md_lines = []
md_lines.append(f"\n## ADIM 7: NORMALÄ°ZASYON VE TRAIN-TEST SPLIT âœ…\n")
md_lines.append(f"**Tarih:** {now}\n\n")

md_lines.append("### ğŸ“Š Veri Seti Ã–zeti\n")
md_lines.append(f"- **Toplam veri:** {len(df_valid):,} satÄ±r (jail>0 olanlar)")
md_lines.append(f"- **Feature sayÄ±sÄ±:** {X_train.shape[1]}")
md_lines.append(f"- **Hedef deÄŸiÅŸken:** 2 (jail, release)")
md_lines.append(f"- **Ã‡Ä±karÄ±lan kayÄ±t:** {removed_len:,} (jail=0 veya NaN)\n")

md_lines.append("### ğŸ”€ Train-Test Split\n")
md_lines.append("```")
md_lines.append(f"Train Set:")
md_lines.append(f"  â€¢ X_train: {X_train.shape[0]:,} satÄ±r Ã— {X_train.shape[1]} feature")
md_lines.append(f"  â€¢ y_train: {y_train.shape[0]:,} satÄ±r Ã— {y_train.shape[1]-1} target (+1 category)")  # -1 Ã§Ã¼nkÃ¼ category geÃ§ici
md_lines.append(f"  â€¢ Oran: %{len(X_train)/len(df_valid)*100:.1f}")
md_lines.append("")
md_lines.append(f"Test Set:")
md_lines.append(f"  â€¢ X_test: {X_test.shape[0]:,} satÄ±r Ã— {X_test.shape[1]} feature")
md_lines.append(f"  â€¢ y_test: {y_test.shape[0]:,} satÄ±r Ã— {y_test.shape[1]-1} target")
md_lines.append(f"  â€¢ Oran: %{len(X_test)/len(df_valid)*100:.1f}")
md_lines.append("```\n")

md_lines.append("### âš™ï¸ Normalizasyon\n")
md_lines.append("- **YÃ¶ntem:** StandardScaler (sklearn)")
md_lines.append("- **Ä°ÅŸlem:** mean=0, std=1")
md_lines.append(f"- **Normalize edilen kolon:** {len(numeric_cols)}")
md_lines.append("- **Scaler kaydedildi:** `model_data/scaler.pkl` (deployment iÃ§in)\n")

md_lines.append("### ğŸ¯ Stratification (Class Imbalance YÃ¶netimi)\n")
md_lines.append("Ceza kategorilerine gÃ¶re stratified split uygulandÄ±:\n")
md_lines.append("**Train Set:**")
md_lines.append("```")
for cat, count in train_cats.items():
    pct = count / len(y_train) * 100
    md_lines.append(f"â€¢ {cat}: {count:,} (%{pct:.2f})")
md_lines.append("```\n")
md_lines.append("**Test Set:**")
md_lines.append("```")
for cat, count in test_cats.items():
    pct = count / len(y_test) * 100
    md_lines.append(f"â€¢ {cat}: {count:,} (%{pct:.2f})")
md_lines.append("```\n")

md_lines.append("### ğŸ“Š Hedef DeÄŸiÅŸken Ä°statistikleri (Train)\n")
md_lines.append("**jail (Hapis SÃ¼resi - GÃ¼n):**")
md_lines.append("```")
md_lines.append(f"â€¢ Ortalama: {y_train['jail'].mean():.2f} gÃ¼n")
md_lines.append(f"â€¢ Median: {y_train['jail'].median():.2f} gÃ¼n")
md_lines.append(f"â€¢ Std Sapma: {y_train['jail'].std():.2f} gÃ¼n")
md_lines.append(f"â€¢ Min: {y_train['jail'].min():.0f} gÃ¼n")
md_lines.append(f"â€¢ Max: {y_train['jail'].max():.0f} gÃ¼n")
md_lines.append("```\n")

md_lines.append("### ğŸ’¾ Kaydedilen Dosyalar\n")
md_lines.append("```")
md_lines.append("model_data/")
md_lines.append("  â”œâ”€â”€ X_train.csv (train features)")
md_lines.append("  â”œâ”€â”€ X_test.csv (test features)")
md_lines.append("  â”œâ”€â”€ y_train.csv (train targets)")
md_lines.append("  â”œâ”€â”€ y_test.csv (test targets)")
md_lines.append("  â”œâ”€â”€ scaler.pkl (StandardScaler objesi)")
md_lines.append("  â””â”€â”€ feature_names.txt (feature isimleri)")
md_lines.append("```\n")

md_lines.append("### âœ… Ã–nemli Notlar\n")
md_lines.append("- âœ… Veri normalize edildi (XGBoost iÃ§in optimal)")
md_lines.append("- âœ… Stratified split ile class imbalance dengelendi")
md_lines.append("- âœ… Scaler kaydedildi (deployment'ta kullanÄ±lacak)")
md_lines.append("- âœ… Feature names kaydedildi (model yorumlama iÃ§in)")
md_lines.append("- âœ… Train/test setleri hazÄ±r â†’ Model eÄŸitimine baÅŸlanabilir!\n")

md_lines.append("---\n")

# Dosyaya ekle
with open(SONUCLAR_PATH, 'a', encoding='utf-8') as f:
    f.write('\n'.join(md_lines))

print(f"âœ… SONUCLAR.md gÃ¼ncellendi: {SONUCLAR_PATH}")

print("\n" + "=" * 70)
print("âœ… ADIM 7 TAMAMLANDI!")
print("=" * 70)
print(f"\nğŸ“Œ Sonraki adÄ±m: XGBoost Model EÄŸitimi")
print(f"ğŸ“Œ HazÄ±r dosyalar: {OUTPUT_DIR}/")
