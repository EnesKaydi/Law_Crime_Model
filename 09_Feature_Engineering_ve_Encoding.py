"""
09_Feature_Engineering_ve_Encoding.py

Bu script:
- Final dataset'i yÃ¼kler ve model iÃ§in hazÄ±rlar
- Kategorik deÄŸiÅŸkenleri encode eder (Label Encoding & One-Hot Encoding)
- Gereksiz kolonlarÄ± Ã§Ä±karÄ±r (ID'ler, multicollinearity olanlar)
- Feature engineering yapar (yeni Ã¶zellikler tÃ¼retir)
- Eksik deÄŸerleri yÃ¶netir (imputation)
- Feature selection yapar (dÃ¼ÅŸÃ¼k korelasyonlu Ã¶zellikleri Ã§Ä±karÄ±r)
- Ä°ÅŸlenmiÅŸ veriyi kaydeder: `wcld_Processed_For_Model.csv`
- TÃ¼m adÄ±mlarÄ± SONUCLAR.md'ye kaydeder

KullanÄ±m:
    /Users/muhammedeneskaydi/PycharmProjects/LAW/.venv/bin/python 09_Feature_Engineering_ve_Encoding.py

Notlar:
- Tez iÃ§in kritik adÄ±m - her iÅŸlem dokÃ¼mante edilmiÅŸtir
- Encoding stratejisi: Binary iÃ§in Label, Multi-class iÃ§in One-Hot
- Multicollinearity Ã§iftlerinden biri Ã§Ä±karÄ±lÄ±r (VIF kontrolÃ¼)
"""

import os
from datetime import datetime
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# --- Ayarlar ---
BASE_DIR = "/Users/muhammedeneskaydi/PycharmProjects/LAW"
FINAL_CSV = os.path.join(BASE_DIR, "wcld_Final_Dataset.csv")
OUTPUT_CSV = os.path.join(BASE_DIR, "wcld_Processed_For_Model.csv")
SONUCLAR_PATH = os.path.join(BASE_DIR, "SONUCLAR.md")

print("=" * 70)
print("ADIM 6: FEATURE ENGINEERING VE ENCODING")
print("=" * 70)

# --- Veri YÃ¼kleme ---
print(f"\nğŸ“‚ Veri yÃ¼kleniyor: {FINAL_CSV}")
df = pd.read_csv(FINAL_CSV)
print(f"âœ… Veri yÃ¼klendi. SatÄ±r: {len(df):,}, Kolon: {len(df.columns)}")
original_shape = df.shape

# --- Ä°ÅŸlem Takibi ---
operations_log = []

# ===== 1. GEREKSIZ KOLONLARI Ã‡IKAR =====
print("\n" + "=" * 70)
print("1. GEREKSIZ KOLONLARI Ã‡IKARMA")
print("=" * 70)

# ID kolonlarÄ± (model iÃ§in gereksiz)
id_cols = ['new_id', 'judge_id', 'county', 'zip']
print(f"\n  ğŸ—‘ï¸ ID kolonlarÄ± Ã§Ä±karÄ±lÄ±yor: {id_cols}")
df = df.drop(columns=id_cols, errors='ignore')
operations_log.append(f"ID kolonlarÄ± Ã§Ä±karÄ±ldÄ±: {id_cols}")

# Train/test split kolonlarÄ± (veri sÄ±zÄ±ntÄ±sÄ± olabilir)
split_cols = ['train_test_split_caselevel', 'train_test_split_deflevel']
print(f"  ğŸ—‘ï¸ Split kolonlarÄ± Ã§Ä±karÄ±lÄ±yor: {split_cols}")
df = df.drop(columns=split_cols, errors='ignore')
operations_log.append(f"Split kolonlarÄ± Ã§Ä±karÄ±ldÄ±: {split_cols}")

# ===== 2. MULTICOLLINEARÄ°TY YÃ–NETÄ°MÄ° =====
print("\n" + "=" * 70)
print("2. MULTICOLLINEARÄ°TY YÃ–NETÄ°MÄ°")
print("=" * 70)

# EDA'da tespit edilen yÃ¼ksek korelasyonlu Ã§iftler
# (probation-release: 1.0, age_offense-age_judge: 0.996, vb.)
multicollinear_pairs = [
    ('probation', 'release', 1.0, 'keep_release'),  # release'i tut (daha genel)
    ('age_offense', 'age_judge', 0.996, 'keep_age_offense'),  # suÃ§lu yaÅŸÄ± Ã¶nemli
    ('avg_hist_jail', 'median_hist_jail', 0.988, 'keep_median_hist_jail'),  # medyan daha robust
    ('min_hist_jail', 'avg_hist_jail', 0.916, 'keep_avg_hist_jail'),  # ortalama daha bilgilendirici
]

print("\n  âš ï¸ YÃ¼ksek korelasyonlu Ã§iftlerden biri Ã§Ä±karÄ±lÄ±yor:")
for feat1, feat2, corr, action in multicollinear_pairs:
    keep = action.split('_', 1)[1]
    drop = feat1 if keep == feat2 else feat2
    
    if drop in df.columns:
        print(f"    â€¢ {feat1} â†” {feat2} (r={corr:.3f}) â†’ {drop} Ã‡IKARILDI")
        df = df.drop(columns=[drop])
        operations_log.append(f"Multicollinearity: {drop} Ã§Ä±karÄ±ldÄ± (r={corr:.3f} with {keep})")

# ===== 3. HEDEF DEÄÄ°ÅKENLERÄ° AYIR =====
print("\n" + "=" * 70)
print("3. HEDEF DEÄÄ°ÅKENLERÄ° AYIRMA")
print("=" * 70)

# Hedef deÄŸiÅŸkenler
target_vars = ['jail', 'release']  # probation Ã§Ä±karÄ±ldÄ± (release ile aynÄ±)
print(f"\n  ğŸ¯ Hedef deÄŸiÅŸkenler: {target_vars}")

# Hedef deÄŸiÅŸkenleri baÅŸka bir DataFrame'e kaydet
df_targets = df[target_vars].copy()
print(f"  âœ… Hedef deÄŸiÅŸkenler ayrÄ±ldÄ±: {df_targets.shape}")

# Hedef deÄŸiÅŸkenleri ana DataFrame'den Ã§Ä±kar (sonra geri ekleyeceÄŸiz)
df_features = df.drop(columns=target_vars)
operations_log.append(f"Hedef deÄŸiÅŸkenler ayrÄ±ldÄ±: {target_vars}")

# ===== 4. KATEGORÄ°K DEÄÄ°ÅKENLERÄ° ENCODE ETME =====
print("\n" + "=" * 70)
print("4. KATEGORÄ°K DEÄÄ°ÅKENLERÄ° ENCODING")
print("=" * 70)

# Kategorik kolonlarÄ± tespit et
categorical_cols = df_features.select_dtypes(include=['object']).columns.tolist()
print(f"\n  ğŸ“‹ {len(categorical_cols)} kategorik kolon bulundu: {categorical_cols}")

encoding_info = {}

# --- 4.1 Binary Kategorik DeÄŸiÅŸkenler (Label Encoding) ---
print("\n  ğŸ”¢ Binary deÄŸiÅŸkenler iÃ§in Label Encoding:")

# sex: M/F â†’ 0/1
if 'sex' in df_features.columns:
    le_sex = LabelEncoder()
    df_features['sex_encoded'] = le_sex.fit_transform(df_features['sex'].fillna('Unknown'))
    print(f"    â€¢ sex: {list(le_sex.classes_)} â†’ {list(range(len(le_sex.classes_)))}")
    encoding_info['sex'] = {'type': 'LabelEncoder', 'classes': list(le_sex.classes_)}
    df_features = df_features.drop(columns=['sex'])

# violent_crime: zaten 0/1 (sayÄ±sal)
print("    â€¢ violent_crime: Zaten binary (0/1) âœ…")

# --- 4.2 Multi-class Kategorik DeÄŸiÅŸkenler (One-Hot Encoding) ---
print("\n  ğŸ”„ Multi-class deÄŸiÅŸkenler iÃ§in One-Hot Encoding:")

# race: Caucasian, African American, Hispanic, vb.
if 'race' in df_features.columns:
    race_dummies = pd.get_dummies(df_features['race'], prefix='race', drop_first=True)
    print(f"    â€¢ race: {df_features['race'].nunique()} kategori â†’ {len(race_dummies.columns)} dummy")
    encoding_info['race'] = {
        'type': 'OneHot', 
        'categories': df_features['race'].unique().tolist(),
        'dummies': race_dummies.columns.tolist()
    }
    df_features = pd.concat([df_features, race_dummies], axis=1)
    df_features = df_features.drop(columns=['race'])

# case_type: Felony, Misdemeanor, Criminal Traffic
if 'case_type' in df_features.columns:
    case_dummies = pd.get_dummies(df_features['case_type'], prefix='case_type', drop_first=True)
    print(f"    â€¢ case_type: {df_features['case_type'].nunique()} kategori â†’ {len(case_dummies.columns)} dummy")
    encoding_info['case_type'] = {
        'type': 'OneHot',
        'categories': df_features['case_type'].unique().tolist(),
        'dummies': case_dummies.columns.tolist()
    }
    df_features = pd.concat([df_features, case_dummies], axis=1)
    df_features = df_features.drop(columns=['case_type'])

# wcisclass: Ã‡OK FAZLA KATEGORÄ° (500+) â†’ Frequency Encoding
if 'wcisclass' in df_features.columns:
    print(f"    â€¢ wcisclass: {df_features['wcisclass'].nunique()} kategori (Ã§ok fazla!)")
    print("      â†’ Frequency Encoding uygulanÄ±yor (kategori frekansÄ± ile encode)")
    
    freq_map = df_features['wcisclass'].value_counts(normalize=True).to_dict()
    df_features['wcisclass_freq'] = df_features['wcisclass'].map(freq_map).fillna(0)
    
    encoding_info['wcisclass'] = {
        'type': 'FrequencyEncoding',
        'unique_categories': df_features['wcisclass'].nunique()
    }
    df_features = df_features.drop(columns=['wcisclass'])

# all_races: Benzer race ile - frequency encoding
if 'all_races' in df_features.columns:
    freq_map_races = df_features['all_races'].value_counts(normalize=True).to_dict()
    df_features['all_races_freq'] = df_features['all_races'].map(freq_map_races).fillna(0)
    df_features = df_features.drop(columns=['all_races'])
    encoding_info['all_races'] = {'type': 'FrequencyEncoding'}

operations_log.append(f"Kategorik encoding tamamlandÄ±: {len(encoding_info)} deÄŸiÅŸken")

# ===== 5. EKSÄ°K DEÄER YÃ–NETÄ°MÄ° =====
print("\n" + "=" * 70)
print("5. EKSÄ°K DEÄER YÃ–NETÄ°MÄ° (IMPUTATION)")
print("=" * 70)

# Eksik deÄŸerleri kontrol et
missing_counts = df_features.isnull().sum()
missing_cols = missing_counts[missing_counts > 0].sort_values(ascending=False)

if len(missing_cols) > 0:
    print(f"\n  âš ï¸ {len(missing_cols)} kolonda eksik deÄŸer var:")
    for col, count in missing_cols.head(10).items():
        pct = count / len(df_features) * 100
        print(f"    â€¢ {col}: {count:,} (%{pct:.2f})")
    
    # SayÄ±sal deÄŸiÅŸkenler iÃ§in median imputation
    print("\n  ğŸ”§ Eksik deÄŸerler median ile doldurulacak (XGBoost eksik deÄŸer ile Ã§alÄ±ÅŸÄ±r ama temizlemek daha iyi)")
    
    imputer = SimpleImputer(strategy='median')
    numeric_cols = df_features.select_dtypes(include=[np.number]).columns
    df_features[numeric_cols] = imputer.fit_transform(df_features[numeric_cols])
    
    operations_log.append(f"Eksik deÄŸerler median ile dolduruldu: {len(missing_cols)} kolon")
    print(f"  âœ… Eksik deÄŸerler dolduruldu")
else:
    print("  âœ… Eksik deÄŸer yok!")

# ===== 6. FEATURE ENGINEERING (YENÄ° Ã–ZELLÄ°KLER) =====
print("\n" + "=" * 70)
print("6. FEATURE ENGINEERING (YENÄ° Ã–ZELLÄ°KLER TÃœRETME)")
print("=" * 70)

print("\n  âš™ï¸ Yeni Ã¶zellikler oluÅŸturuluyor:")

# 6.1 Toplam suÃ§ geÃ§miÅŸi
if 'prior_felony' in df_features.columns and 'prior_misdemeanor' in df_features.columns:
    df_features['total_prior_crimes'] = df_features['prior_felony'] + df_features['prior_misdemeanor']
    print("    â€¢ total_prior_crimes = prior_felony + prior_misdemeanor")

# 6.2 AÄŸÄ±r suÃ§ oranÄ±
if 'prior_felony' in df_features.columns and 'total_prior_crimes' in df_features.columns:
    df_features['felony_ratio'] = df_features['prior_felony'] / (df_features['total_prior_crimes'] + 1)
    print("    â€¢ felony_ratio = prior_felony / (total_prior_crimes + 1)")

# 6.3 YaÅŸ grubu (kategorik â†’ sayÄ±sal)
if 'age_offense' in df_features.columns:
    df_features['age_group_young'] = (df_features['age_offense'] < 25).astype(int)
    df_features['age_group_old'] = (df_features['age_offense'] > 60).astype(int)
    print("    â€¢ age_group_young (<25), age_group_old (>60)")

# 6.4 YÃ¼ksek risk skoru (violent + recidivism)
if 'violent_crime' in df_features.columns and 'recid_180d' in df_features.columns:
    df_features['high_risk_score'] = (df_features['violent_crime'].fillna(0) + 
                                       df_features['recid_180d'].fillna(0))
    print("    â€¢ high_risk_score = violent_crime + recid_180d")

# 6.5 Mahalle sosyoekonomik skoru (birleÅŸik)
socio_cols = ['pct_college', 'med_hhinc', 'pct_food_stamps']
available_socio = [col for col in socio_cols if col in df_features.columns]
if len(available_socio) >= 2:
    # Normalize edip birleÅŸtir
    df_features['socioeconomic_score'] = 0
    for col in available_socio:
        normalized = (df_features[col] - df_features[col].mean()) / df_features[col].std()
        if col == 'pct_food_stamps':
            normalized = -normalized  # Negatif etki (food stamps yÃ¼ksek = dÃ¼ÅŸÃ¼k sosyoekonomik)
        df_features['socioeconomic_score'] += normalized
    print(f"    â€¢ socioeconomic_score (birleÅŸik: {available_socio})")

operations_log.append("Feature engineering tamamlandÄ±: 6 yeni Ã¶zellik")

# ===== 7. DÃœÅÃœK Ã–NEM DEÄERLÄ° Ã–ZELLÄ°KLERÄ° Ã‡IKAR =====
print("\n" + "=" * 70)
print("7. DÃœÅÃœK KORELASYONLU Ã–ZELLÄ°KLERÄ° Ã‡IKARMA")
print("=" * 70)

# Hedef deÄŸiÅŸkeni geri ekle (geÃ§ici)
df_temp = pd.concat([df_features, df_targets], axis=1)

# Sadece jail ile korelasyonu Ã§ok dÃ¼ÅŸÃ¼k olanlarÄ± Ã§Ä±kar
if 'jail' in df_temp.columns:
    numeric_features = df_features.select_dtypes(include=[np.number]).columns
    correlations = df_temp[numeric_features].corrwith(df_temp['jail']).abs()
    
    low_corr_features = correlations[correlations < 0.01].index.tolist()
    
    if len(low_corr_features) > 0:
        print(f"\n  âš ï¸ {len(low_corr_features)} Ã¶zellik jail ile Ã§ok dÃ¼ÅŸÃ¼k korelasyonlu (|r| < 0.01):")
        for feat in low_corr_features[:10]:
            print(f"    â€¢ {feat}: r = {correlations[feat]:.4f}")
        
        print(f"\n  ğŸ—‘ï¸ Bu Ã¶zellikler Ã§Ä±karÄ±lacak (model iÃ§in gereksiz)")
        df_features = df_features.drop(columns=low_corr_features, errors='ignore')
        operations_log.append(f"DÃ¼ÅŸÃ¼k korelasyonlu {len(low_corr_features)} Ã¶zellik Ã§Ä±karÄ±ldÄ±")
    else:
        print("  âœ… TÃ¼m Ã¶zellikler yeterli korelasyona sahip")

# GeÃ§ici DataFrame'i temizle
del df_temp

# ===== 8. FÄ°NAL VERÄ° SETÄ° BÄ°RLEÅTÄ°RME =====
print("\n" + "=" * 70)
print("8. FÄ°NAL VERÄ° SETÄ° BÄ°RLEÅTÄ°RME")
print("=" * 70)

# Hedef deÄŸiÅŸkenleri geri ekle
df_final = pd.concat([df_features, df_targets], axis=1)

print(f"\n  âœ… Final veri seti oluÅŸturuldu:")
print(f"    â€¢ SatÄ±r sayÄ±sÄ±: {len(df_final):,}")
print(f"    â€¢ Feature sayÄ±sÄ±: {len(df_features.columns)}")
print(f"    â€¢ Hedef deÄŸiÅŸken sayÄ±sÄ±: {len(df_targets.columns)}")
print(f"    â€¢ Toplam kolon: {len(df_final.columns)}")

# ===== 9. VERÄ° SETÄ°NÄ° KAYDET =====
print("\n" + "=" * 70)
print("9. Ä°ÅLENMÄ°Å VERÄ°YÄ° KAYDETME")
print("=" * 70)

print(f"\n  ğŸ’¾ Ä°ÅŸlenmiÅŸ veri kaydediliyor: {OUTPUT_CSV}")
df_final.to_csv(OUTPUT_CSV, index=False)

file_size_mb = os.path.getsize(OUTPUT_CSV) / 1024**2
print(f"  âœ… KayÄ±t tamamlandÄ±! Dosya boyutu: {file_size_mb:.2f} MB")

# ===== 10. SONUÃ‡LAR.MD'YE EKLEME =====
print("\n" + "=" * 70)
print("10. SONUCLAR.MD GÃœNCELLEME")
print("=" * 70)

now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

md_lines = []
md_lines.append(f"\n## ADIM 6: FEATURE ENGINEERING VE ENCODING âœ…\n")
md_lines.append(f"**Tarih:** {now}\n\n")

md_lines.append("### ğŸ“Š Ä°ÅŸlem Ã–zeti\n")
md_lines.append(f"- **Orijinal boyut:** {original_shape[0]:,} satÄ±r Ã— {original_shape[1]} kolon")
md_lines.append(f"- **Final boyut:** {df_final.shape[0]:,} satÄ±r Ã— {df_final.shape[1]} kolon")
md_lines.append(f"- **Feature sayÄ±sÄ±:** {len(df_features.columns)}")
md_lines.append(f"- **Hedef deÄŸiÅŸken:** {len(df_targets.columns)} (jail, release)\n")

md_lines.append("### ğŸ”§ YapÄ±lan Ä°ÅŸlemler\n")
md_lines.append("```")
for i, op in enumerate(operations_log, 1):
    md_lines.append(f"{i}. {op}")
md_lines.append("```\n")

md_lines.append("### ğŸ“‹ Encoding DetaylarÄ±\n")
for var, info in encoding_info.items():
    md_lines.append(f"**{var}:**")
    md_lines.append(f"- Encoding Tipi: {info['type']}")
    if 'classes' in info:
        md_lines.append(f"- SÄ±nÄ±flar: {info['classes']}")
    if 'dummies' in info:
        md_lines.append(f"- OluÅŸturulan dummy sayÄ±sÄ±: {len(info['dummies'])}")
    md_lines.append("")

md_lines.append("### âš™ï¸ Yeni OluÅŸturulan Ã–zellikler\n")
md_lines.append("1. `total_prior_crimes`: Toplam suÃ§ geÃ§miÅŸi")
md_lines.append("2. `felony_ratio`: AÄŸÄ±r suÃ§ oranÄ±")
md_lines.append("3. `age_group_young` / `age_group_old`: YaÅŸ grubu binary")
md_lines.append("4. `high_risk_score`: Åiddet + tekrar suÃ§ skoru")
md_lines.append("5. `socioeconomic_score`: Mahalle sosyoekonomik skoru")
md_lines.append("6. `wcisclass_freq` / `all_races_freq`: Frequency encoding\n")

md_lines.append("### ğŸ’¾ Kaydedilen Dosya\n")
md_lines.append(f"- **Dosya:** `wcld_Processed_For_Model.csv`")
md_lines.append(f"- **Boyut:** {file_size_mb:.2f} MB")
md_lines.append(f"- **KullanÄ±m:** XGBoost model eÄŸitimi iÃ§in hazÄ±r\n")

md_lines.append("### âœ… Ã–nemli Notlar\n")
md_lines.append("- âœ… TÃ¼m kategorik deÄŸiÅŸkenler sayÄ±sal formata Ã§evrildi")
md_lines.append("- âœ… Multicollinearity temizlendi (VIF riski azaltÄ±ldÄ±)")
md_lines.append("- âœ… Eksik deÄŸerler yÃ¶netildi (median imputation)")
md_lines.append("- âœ… Feature engineering ile 6 yeni Ã¶zellik eklendi")
md_lines.append("- âœ… DÃ¼ÅŸÃ¼k korelasyonlu Ã¶zellikler Ã§Ä±karÄ±ldÄ±")
md_lines.append("- âœ… Veri model eÄŸitimine hazÄ±r!\n")

md_lines.append("---\n")

# Dosyaya ekle
with open(SONUCLAR_PATH, 'a', encoding='utf-8') as f:
    f.write('\n'.join(md_lines))

print(f"âœ… SONUCLAR.md gÃ¼ncellendi: {SONUCLAR_PATH}")

print("\n" + "=" * 70)
print("âœ… ADIM 6 TAMAMLANDI!")
print("=" * 70)
print(f"\nğŸ“Œ Sonraki adÄ±m: Veri Normalizasyonu & Train-Test Split")
print(f"ğŸ“Œ Model eÄŸitimine hazÄ±r: {OUTPUT_CSV}")
