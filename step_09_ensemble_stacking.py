
import pandas as pd
import numpy as np
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor
from sklearn.model_selection import KFold, cross_val_predict
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import LabelEncoder
import joblib
from pathlib import Path
import warnings

# Ayarlar
warnings.filterwarnings('ignore')

VERI_YOLU = "/Users/muhammedeneskaydi/PycharmProjects/LAW/wcld.csv"
OUTPUT_DIR = Path("outputs/model_ensemble")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
MODEL_DIR = Path("model_data_ensemble")
MODEL_DIR.mkdir(parents=True, exist_ok=True)

def train_ensemble():
    print(f"ğŸ“‚ Veri yÃ¼kleniyor: {VERI_YOLU}")
    try:
        df = pd.read_csv(VERI_YOLU, low_memory=False)
    except FileNotFoundError:
        print("âŒ HATA: Dosya bulunamadÄ±!")
        return

    # 1. TEMÄ°ZLÄ°K
    if 'jail' not in df.columns: return
    df = df[df['jail'] > 300].copy()
    ust_sinir = df['jail'].quantile(0.995)
    df = df[df['jail'] <= ust_sinir].copy()
    print(f"âœ… Analiz Verisi: {df.shape[0]} satÄ±r")
    
    # 2. FEATURE ENGINEERING
    y = np.log1p(df['jail'])
    
    # Yeni Ã–zellikler Eklendi: county, case_type
    features = [
        'highest_severity', 'violent_crime', 'is_recid_new', 'year',
        'wcisclass', 'release', 'max_hist_jail', 'pct_male', 'judge_id',
        'age_judge', 'age_offense', 'pct_black', 'sex', 'race', 
        'prior_felony', 'prior_misdemeanor', 'prior_criminal_traffic',
        'avg_hist_jail', 'median_hist_jail', 'min_hist_jail',
        'county', 'case_type' # YENÄ°
    ]
    prior_severity_cols = [c for c in df.columns if 'prior_charges_severity' in c]
    features.extend(prior_severity_cols)
    
    available_features = [f for f in features if f in df.columns]
    X = df[available_features].copy()
    
    # KATEGORÄ°K Ä°ÅLEMLERÄ°
    cat_features = []
    
    # 1. AdÄ±m: Eksik Doldurma (Unknown)
    for col in X.columns:
        if X[col].dtype == 'object':
            X[col] = X[col].fillna("Unknown").astype(str)
            cat_features.append(col)
        elif X[col].dtype.name == 'category':
             X[col] = X[col].astype(str).fillna("Unknown")
             cat_features.append(col)
             
    # Judge ID, County, Zip vb string olmalÄ±
    for col in ['judge_id', 'county', 'zip']:
        if col in X.columns and col not in cat_features:
            X[col] = X[col].astype(str)
            cat_features.append(col)

    # 2. AdÄ±m: Label Encoding (XGBoost ve LightGBM iÃ§in)
    # CatBoost kendi halleder ama diÄŸerleri sayÄ± ister.
    X_encoded = X.copy()
    encoders = {}
    for col in cat_features:
        le = LabelEncoder()
        # Bilinmeyen deÄŸerleri handle etmek zordur LabelEncoder ile, o yÃ¼zden basit fit
        X_encoded[col] = le.fit_transform(X[col].astype(str))
        encoders[col] = le
        
    print(f"ğŸ“Œ Kategorik DeÄŸiÅŸkenler: {cat_features}")

    # 3. K-FOLD STACKING
    # Veriyi bÃ¶lmeden tÃ¼m veri Ã¼zerinde CV ile tahmin Ã¼retip metamodel eÄŸitelim
    # (GerÃ§ek bir projede hold-out test set ayrÄ±lmalÄ±, burada R2 maksimizasyonu iÃ§in CV yapÄ±yoruz)
    
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    
    print("\nğŸš€ MODEL 1: CatBoost EÄŸitiliyor (K-Fold)...")
    cat_model = CatBoostRegressor(
        iterations=1000, learning_rate=0.05, depth=8, cat_features=cat_features, verbose=0, random_seed=42
    )
    # CatBoost kategorik veriyi sever, X'i (encoded olmayan) veriyoruz
    cat_preds = cross_val_predict(cat_model, X, y, cv=kf, n_jobs=-1)
    
    print("ğŸš€ MODEL 2: XGBoost EÄŸitiliyor (K-Fold)...")
    xgb_model = xgb.XGBRegressor(
        n_estimators=1000, learning_rate=0.05, max_depth=6, n_jobs=-1, random_state=42
    )
    # XGBoost encoded sever
    xgb_preds = cross_val_predict(xgb_model, X_encoded, y, cv=kf, n_jobs=-1)
    
    print("ğŸš€ MODEL 3: LightGBM EÄŸitiliyor (K-Fold)...")
    lgb_model = lgb.LGBMRegressor(
        n_estimators=1000, learning_rate=0.05, max_depth=8, n_jobs=-1, random_state=42, verbose=-1
    )
    lgb_preds = cross_val_predict(lgb_model, X_encoded, y, cv=kf, n_jobs=-1)
    
    # 4. STACKING (EÅLEÅTÄ°RME)
    print("\nğŸ—ï¸ Stacking (Meta-Model) EÄŸitiliyor...")
    
    stacked_X = pd.DataFrame({
        'CatBoost': cat_preds,
        'XGBoost': xgb_preds,
        'LightGBM': lgb_preds
    })
    
    # Meta Model: Ridge Regression (Overfit olmasÄ±n diye)
    meta_model = Ridge(alpha=1.0)
    meta_model.fit(stacked_X, y)
    
    final_preds_log = meta_model.predict(stacked_X)
    final_preds = np.expm1(final_preds_log)
    y_orig = np.expm1(y)
    
    # 5. DEÄERLENDÄ°RME
    r2_log = r2_score(y, final_preds_log)
    r2_orig = r2_score(y_orig, final_preds)
    mae = mean_absolute_error(y_orig, final_preds)
    rmse = np.sqrt(mean_squared_error(y_orig, final_preds))
    
    print("\nğŸ“Š ENSEMBLE (STACKING) SONUÃ‡LARI:")
    print(f"ğŸ”¹ R2 Score (Log Scale): {r2_log:.4f}")
    print(f"ğŸ”¹ R2 Score (Original): {r2_orig:.4f}")
    print(f"ğŸ”¹ MAE: {mae:.2f} gÃ¼n")
    print(f"ğŸ”¹ RMSE: {rmse:.2f} gÃ¼n")
    
    print("\nâš–ï¸ Model AÄŸÄ±rlÄ±klarÄ± (Hangi model ne kadar etkili?):")
    for name, coef in zip(stacked_X.columns, meta_model.coef_):
        print(f"  â€¢ {name}: {coef:.4f}")
        
    # Her bir modelin tekil baÅŸarÄ±sÄ±
    print("\nğŸ” Tekil Model BaÅŸarÄ±larÄ± (Log R2):")
    print(f"  â€¢ CatBoost: {r2_score(y, cat_preds):.4f}")
    print(f"  â€¢ XGBoost: {r2_score(y, xgb_preds):.4f}")
    print(f"  â€¢ LightGBM: {r2_score(y, lgb_preds):.4f}")
    
    # Modeli Kaydet (Meta Model ve Base Modellerin Full Data ile EÄŸitilmesi LazÄ±m Ã¼retim iÃ§in)
    # Åimdilik analiz amaÃ§lÄ± skorlarÄ± gÃ¶steriyoruz.
    
    if r2_log > 0.80:
        print("\nğŸ‰ TEBRÄ°KLER! %80 BARAJI AÅILDI!")
    else:
        print("\nâš ï¸ Hala %80 altÄ±ndayÄ±z ama yaklaÅŸtÄ±k.")

if __name__ == "__main__":
    train_ensemble()
