"""
ğŸ”¬ SCIENTIFIC ANALYSIS: RÂ² LIMITATION ROOT CAUSE
================================================

Bu script, High Severity Model'in neden %38'de takÄ±lÄ± kaldÄ±ÄŸÄ±nÄ± BÄ°LÄ°MSEL OLARAK analiz eder.

Rastgele deneme yanÄ±lma DEÄÄ°L, sistematik bilimsel yaklaÅŸÄ±m:
1. Variance Decomposition (Varyans AyrÄ±ÅŸtÄ±rma)
2. Information Theory Analysis (Bilgi Teorisi)
3. Feature Correlation Matrix (Ã–zellik Ä°liÅŸkileri)
4. Error Source Categorization (Hata KaynaklarÄ±)
5. Theoretical Upper Bound Calculation (Teorik Ãœst Limit)

Hedef: %50 RÂ²'ye ulaÅŸmak iÃ§in NELER GEREKLÄ° olduÄŸunu kesin olarak belirlemek.
"""

import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mutual_info_score
from sklearn.feature_selection import mutual_info_regression
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from scipy import stats
from scipy.stats import spearmanr, pearsonr
import joblib

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Paths
VERI_YOLU = "/Users/muhammedeneskaydi/PycharmProjects/LAW/wcld.csv"
OUTPUT_DIR = Path("../outputs/scientific_analysis")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

THRESHOLD = 3000
RANDOM_STATE = 42


def load_and_prepare_data():
    """Veriyi yÃ¼kle ve hazÄ±rla"""
    print("ğŸ“‚ Veri yÃ¼kleniyor...")
    df = pd.read_csv(VERI_YOLU, low_memory=False)
    
    df = df[df['jail'] > 300].copy()
    ust_sinir = df['jail'].quantile(0.995)
    df = df[df['jail'] <= ust_sinir].copy()
    
    df_high = df[df['jail'] > THRESHOLD].copy()
    
    # Advanced features ekle
    if 'highest_severity' in df_high.columns and 'violent_crime' in df_high.columns:
        df_high['severity_x_violent'] = df_high['highest_severity'] * df_high['violent_crime']
    
    if 'age_judge' in df_high.columns and 'age_offense' in df_high.columns:
        age_j = df_high['age_judge'].fillna(df_high['age_judge'].mean())
        age_o = df_high['age_offense'].fillna(df_high['age_offense'].mean())
        df_high['age_gap'] = age_j - age_o
    
    if 'is_recid_new' in df_high.columns and 'violent_crime' in df_high.columns:
        df_high['violent_recid'] = df_high['is_recid_new'] * df_high['violent_crime']
    
    # Yeni advanced features
    if 'judge_id' in df_high.columns:
        judge_avg = df_high.groupby('judge_id')['jail'].transform('mean')
        df_high['judge_harshness'] = judge_avg
        judge_std = df_high.groupby('judge_id')['jail'].transform('std')
        df_high['judge_consistency'] = judge_std.fillna(0)
    
    if 'county' in df_high.columns:
        county_avg = df_high.groupby('county')['jail'].transform('mean')
        df_high['county_harshness'] = county_avg
    
    if 'wcisclass' in df_high.columns:
        wcis_avg = df_high.groupby('wcisclass')['jail'].transform('mean')
        df_high['wcisclass_severity'] = wcis_avg
    
    if 'judge_id' in df_high.columns and 'wcisclass' in df_high.columns:
        df_high['judge_crime_combo'] = df_high['judge_id'].astype(str) + '_' + df_high['wcisclass'].astype(str)
    
    if 'is_recid_new' in df_high.columns and 'highest_severity' in df_high.columns:
        df_high['recid_severity'] = df_high['is_recid_new'] * df_high['highest_severity']
    
    prior_cols = ['prior_felony', 'prior_misdemeanor', 'prior_criminal_traffic']
    available_prior = [c for c in prior_cols if c in df_high.columns]
    if available_prior:
        df_high['total_prior_score'] = df_high[available_prior].fillna(0).sum(axis=1)
    
    if 'violent_crime' in df_high.columns and 'total_prior_score' in df_high.columns:
        df_high['violent_x_prior'] = df_high['violent_crime'] * df_high['total_prior_score']
    
    if 'age_offense' in df_high.columns and 'violent_crime' in df_high.columns:
        age_normalized = (df_high['age_offense'] - df_high['age_offense'].mean()) / df_high['age_offense'].std()
        df_high['age_risk'] = age_normalized * df_high['violent_crime']
    
    if 'year' in df_high.columns:
        df_high['years_since_2000'] = df_high['year'] - 2000
        df_high['year_squared'] = df_high['years_since_2000'] ** 2
    
    print(f"âœ… High Severity veri hazÄ±r: {len(df_high):,} vaka")
    return df_high


def prepare_features(df):
    """Feature listesini hazÄ±rla"""
    base_features = [
        'highest_severity', 'violent_crime', 'is_recid_new', 'year',
        'wcisclass', 'release', 'max_hist_jail', 'pct_male', 'judge_id',
        'age_judge', 'age_offense', 'pct_black', 'sex', 'race',
        'prior_felony', 'prior_misdemeanor', 'prior_criminal_traffic',
        'avg_hist_jail', 'median_hist_jail', 'min_hist_jail',
        'county', 'case_type', 'zip'
    ]
    base_features.extend([c for c in df.columns if 'prior_charges_severity' in c])
    
    new_features = [
        'severity_x_violent', 'age_gap', 'violent_recid',
        'judge_harshness', 'judge_consistency', 'county_harshness',
        'wcisclass_severity', 'judge_crime_combo', 'recid_severity',
        'total_prior_score', 'violent_x_prior', 'age_risk',
        'years_since_2000', 'year_squared'
    ]
    
    all_features = base_features + new_features
    available_features = [f for f in all_features if f in df.columns]
    
    cat_features = []
    KNOWN_CAT = ['judge_id', 'county', 'zip', 'case_type', 'race', 'sex', 'wcisclass', 'judge_crime_combo']
    
    X = df[available_features].copy()
    
    for col in X.columns:
        if col in KNOWN_CAT or X[col].dtype == 'object' or X[col].dtype.name == 'category':
            X[col] = X[col].fillna("Unknown").astype(str)
            if col not in cat_features:
                cat_features.append(col)
    
    for col in X.columns:
        if col not in cat_features:
            X[col] = X[col].fillna(X[col].mean())
    
    return X, available_features, cat_features


def analysis_1_variance_decomposition(df_high, X, y, cat_features):
    """1. Varyans AyrÄ±ÅŸtÄ±rma Analizi"""
    print("\n" + "="*70)
    print("ğŸ”¬ ANALÄ°Z 1: VARYANS AYRIÅTIRILMASI (Variance Decomposition)")
    print("="*70)
    
    # Total variance
    total_var = y.var()
    print(f"\nğŸ“Š Toplam Varyans (Log Scale): {total_var:.4f}")
    
    # Model eÄŸit
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)
    
    # Kategorik feature'larÄ±n string olduÄŸundan emin ol
    for col in cat_features:
        if col in X_train.columns:
            X_train[col] = X_train[col].astype(str)
            X_test[col] = X_test[col].astype(str)
    
    model = CatBoostRegressor(
        iterations=2000,
        learning_rate=0.02,
        depth=12,
        cat_features=cat_features,
        verbose=0,
        random_seed=RANDOM_STATE
    )
    model.fit(X_train, y_train)
    
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    
    # Variance decomposition
    explained_var = r2 * y_test.var()
    unexplained_var = (1 - r2) * y_test.var()
    
    print(f"\nğŸ“ˆ Varyans AyrÄ±ÅŸtÄ±rmasÄ±:")
    print(f"   â€¢ AÃ§Ä±klanan Varyans: {explained_var:.4f} ({r2*100:.1f}%)")
    print(f"   â€¢ AÃ§Ä±klanamayan Varyans: {unexplained_var:.4f} ({(1-r2)*100:.1f}%)")
    print(f"   â€¢ RÂ² Score: {r2:.4f}")
    
    # Residual analysis
    residuals = y_test - y_pred
    residual_var = residuals.var()
    
    print(f"\nğŸ” Residual (Hata) Analizi:")
    print(f"   â€¢ Residual Variance: {residual_var:.4f}")
    print(f"   â€¢ Residual Std: {residuals.std():.4f}")
    print(f"   â€¢ Residual Mean: {residuals.mean():.4f} (ideal: 0)")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Variance pie chart
    axes[0].pie([explained_var, unexplained_var], 
                labels=[f'AÃ§Ä±klanan\n{r2*100:.1f}%', f'AÃ§Ä±klanamayan\n{(1-r2)*100:.1f}%'],
                autopct='%1.1f%%', startangle=90, colors=['green', 'red'])
    axes[0].set_title('Varyans AyrÄ±ÅŸtÄ±rmasÄ±')
    
    # Residual distribution
    axes[1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
    axes[1].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[1].set_xlabel('Residuals (Hatalar)')
    axes[1].set_ylabel('Frequency')
    axes[1].set_title('Residual DaÄŸÄ±lÄ±mÄ±')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '01_variance_decomposition.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 01_variance_decomposition.png")
    plt.close()
    
    return {
        'r2': r2,
        'explained_var': explained_var,
        'unexplained_var': unexplained_var,
        'residuals': residuals,
        'model': model
    }


def analysis_2_feature_correlations(X, y):
    """2. Feature-Target Korelasyon Analizi"""
    print("\n" + "="*70)
    print("ğŸ”¬ ANALÄ°Z 2: FEATURE-TARGET KORELASYON ANALÄ°ZÄ°")
    print("="*70)
    
    # Sadece numerik feature'lar iÃ§in korelasyon hesapla
    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
    X_numeric = X[numeric_features]
    
    # Pearson correlation
    correlations = []
    for col in numeric_features:
        corr, p_value = pearsonr(X_numeric[col], y)
        correlations.append({
            'feature': col,
            'correlation': abs(corr),
            'corr_raw': corr,
            'p_value': p_value
        })
    
    df_corr = pd.DataFrame(correlations).sort_values('correlation', ascending=False)
    
    print(f"\nğŸ“Š Top 15 En YÃ¼ksek Korelasyonlu Feature'lar:")
    print(f"\n{'Feature':<30} {'Correlation':<15} {'P-value':<15}")
    print("-" * 60)
    for _, row in df_corr.head(15).iterrows():
        print(f"{row['feature']:<30} {row['corr_raw']:<15.4f} {row['p_value']:<15.6f}")
    
    # Mutual Information (Kategorik feature'lar iÃ§in)
    print(f"\nğŸ” Mutual Information Analizi (TÃ¼m Feature'lar):")
    
    # Kategorik feature'larÄ± encode et
    X_encoded = X.copy()
    for col in X_encoded.columns:
        if X_encoded[col].dtype == 'object':
            X_encoded[col] = X_encoded[col].astype('category').cat.codes
    
    mi_scores = mutual_info_regression(X_encoded, y, random_state=RANDOM_STATE)
    mi_df = pd.DataFrame({
        'feature': X.columns,
        'mi_score': mi_scores
    }).sort_values('mi_score', ascending=False)
    
    print(f"\n{'Feature':<30} {'MI Score':<15}")
    print("-" * 45)
    for _, row in mi_df.head(15).iterrows():
        print(f"{row['feature']:<30} {row['mi_score']:<15.4f}")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Top correlations
    top_corr = df_corr.head(15)
    axes[0].barh(top_corr['feature'], top_corr['correlation'], color='steelblue')
    axes[0].set_xlabel('Absolute Correlation')
    axes[0].set_title('Top 15 Feature-Target Correlations')
    axes[0].invert_yaxis()
    
    # Top MI scores
    top_mi = mi_df.head(15)
    axes[1].barh(top_mi['feature'], top_mi['mi_score'], color='coral')
    axes[1].set_xlabel('Mutual Information Score')
    axes[1].set_title('Top 15 Mutual Information Scores')
    axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '02_feature_correlations.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 02_feature_correlations.png")
    plt.close()
    
    return {
        'correlations': df_corr,
        'mutual_info': mi_df
    }


def analysis_3_error_categorization(df_high, X, y, model):
    """3. Hata KaynaklarÄ±nÄ±n Kategorize Edilmesi"""
    print("\n" + "="*70)
    print("ğŸ”¬ ANALÄ°Z 3: HATA KAYNAKLARININ KATEGORÄ°ZE EDÄ°LMESÄ°")
    print("="*70)
    
    # Tahminler
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)
    y_pred = model.predict(X_test)
    
    # Test setindeki orijinal deÄŸerleri al
    test_indices = X_test.index
    df_test = df_high.loc[test_indices].copy()
    
    # HatalarÄ± hesapla
    df_test['y_true'] = y_test.values
    df_test['y_pred'] = y_pred
    df_test['error'] = y_test.values - y_pred
    df_test['abs_error'] = np.abs(df_test['error'])
    df_test['error_pct'] = (df_test['abs_error'] / y_test.values) * 100
    
    # Hata kategorileri
    print(f"\nğŸ“Š Hata DaÄŸÄ±lÄ±mÄ±:")
    print(f"   â€¢ Mean Absolute Error: {df_test['abs_error'].mean():.4f}")
    print(f"   â€¢ Median Absolute Error: {df_test['abs_error'].median():.4f}")
    print(f"   â€¢ 90th Percentile Error: {df_test['abs_error'].quantile(0.9):.4f}")
    print(f"   â€¢ Max Error: {df_test['abs_error'].max():.4f}")
    
    # SuÃ§ tipine gÃ¶re hata analizi
    if 'wcisclass' in df_test.columns:
        print(f"\nğŸ” SuÃ§ SÄ±nÄ±fÄ±na GÃ¶re Ortalama Hata:")
        wcis_errors = df_test.groupby('wcisclass')['abs_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)
        print(wcis_errors.head(10))
    
    # Hakim bazlÄ± hata analizi
    if 'judge_id' in df_test.columns:
        print(f"\nğŸ” Hakim BazlÄ± Ortalama Hata (Top 10 En YÃ¼ksek):")
        judge_errors = df_test.groupby('judge_id')['abs_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)
        print(judge_errors.head(10))
    
    # BÃ¶lge bazlÄ± hata analizi
    if 'county' in df_test.columns:
        print(f"\nğŸ” BÃ¶lge BazlÄ± Ortalama Hata:")
        county_errors = df_test.groupby('county')['abs_error'].agg(['mean', 'count']).sort_values('mean', ascending=False)
        print(county_errors.head(10))
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Error distribution
    axes[0, 0].hist(df_test['error'], bins=50, edgecolor='black', alpha=0.7)
    axes[0, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)
    axes[0, 0].set_xlabel('Prediction Error')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].set_title('Error Distribution')
    
    # Absolute error by jail days
    axes[0, 1].scatter(np.expm1(y_test), df_test['abs_error'], alpha=0.3, s=10)
    axes[0, 1].set_xlabel('True Jail Days')
    axes[0, 1].set_ylabel('Absolute Error (log)')
    axes[0, 1].set_title('Error vs True Value')
    
    # Error percentage distribution
    axes[1, 0].hist(df_test['error_pct'], bins=50, edgecolor='black', alpha=0.7, color='coral')
    axes[1, 0].set_xlabel('Error Percentage (%)')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Percentage Error Distribution')
    
    # Top error cases
    top_errors = df_test.nlargest(20, 'abs_error')
    axes[1, 1].scatter(range(len(top_errors)), top_errors['abs_error'], s=100, color='red')
    axes[1, 1].set_xlabel('Case Index')
    axes[1, 1].set_ylabel('Absolute Error')
    axes[1, 1].set_title('Top 20 Highest Error Cases')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '03_error_categorization.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 03_error_categorization.png")
    plt.close()
    
    return df_test


def analysis_4_theoretical_limit(variance_results, correlation_results):
    """4. Teorik RÂ² Ãœst Limitinin HesaplanmasÄ±"""
    print("\n" + "="*70)
    print("ğŸ”¬ ANALÄ°Z 4: TEORÄ°K RÂ² ÃœST LÄ°MÄ°TÄ° HESAPLAMA")
    print("="*70)
    
    current_r2 = variance_results['r2']
    
    # En yÃ¼ksek korelasyonlu feature'larÄ±n RÂ² potansiyeli
    top_corr = correlation_results['correlations'].head(10)
    max_single_feature_r2 = top_corr['correlation'].max() ** 2
    
    print(f"\nğŸ“Š Mevcut Durum:")
    print(f"   â€¢ Mevcut RÂ²: {current_r2:.4f} ({current_r2*100:.1f}%)")
    print(f"   â€¢ En gÃ¼Ã§lÃ¼ tek feature RÂ²: {max_single_feature_r2:.4f} ({max_single_feature_r2*100:.1f}%)")
    
    # Teorik Ã¼st limit tahmini
    # EÄŸer en iyi 10 feature'Ä±n korelasyonlarÄ± dÃ¼ÅŸÃ¼kse, Ã§ok fazla iyileÅŸme beklenemez
    top_10_avg_corr = top_corr['correlation'].mean()
    estimated_ceiling = min(0.65, top_10_avg_corr ** 1.5)  # Heuristic formula
    
    print(f"\nğŸ¯ Teorik Limit Tahmini:")
    print(f"   â€¢ Top 10 Feature Ortalama Korelasyon: {top_10_avg_corr:.4f}")
    print(f"   â€¢ Tahmini RÂ² Tavan: {estimated_ceiling:.4f} ({estimated_ceiling*100:.1f}%)")
    print(f"   â€¢ Mevcut RÂ² / Tavan: {(current_r2/estimated_ceiling)*100:.1f}%")
    
    # %50 hedefine ulaÅŸmak iÃ§in ne gerekli?
    target_r2 = 0.50
    gap = target_r2 - current_r2
    gap_pct = (gap / current_r2) * 100
    
    print(f"\nğŸš€ %50 Hedefine UlaÅŸmak Ä°Ã§in:")
    print(f"   â€¢ Gerekli Ä°yileÅŸme: {gap:.4f} ({gap_pct:.1f}% artÄ±ÅŸ)")
    print(f"   â€¢ Mevcut â†’ Hedef: {current_r2:.4f} â†’ {target_r2:.4f}")
    
    if target_r2 > estimated_ceiling:
        print(f"\nâš ï¸ UYARI: %50 hedefi tahmini tavandan ({estimated_ceiling*100:.1f}%) YÃœKSEK!")
        print(f"   â†’ Mevcut feature'larla %50'ye ulaÅŸmak ZOR gÃ¶rÃ¼nÃ¼yor.")
        print(f"   â†’ Yeni, gÃ¼Ã§lÃ¼ feature'lar (yÃ¼ksek korelasyonlu) gerekli!")
    else:
        print(f"\nâœ… %50 hedefi teorik olarak MÃœMKÃœN!")
        print(f"   â†’ Daha iyi feature engineering ve model optimization ile ulaÅŸÄ±labilir.")
    
    # GÃ¶rselleÅŸtirme
    fig, ax = plt.subplots(figsize=(10, 6))
    
    labels = ['Mevcut RÂ²', 'Hedef RÂ² (50%)', 'Tahmini Tavan']
    values = [current_r2, target_r2, estimated_ceiling]
    colors = ['blue', 'orange', 'red']
    
    bars = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor='black')
    
    # DeÄŸerleri yazdÄ±r
    for bar, val in zip(bars, values):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{val:.2%}', ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    ax.set_ylabel('RÂ² Score')
    ax.set_title('RÂ² KarÅŸÄ±laÅŸtÄ±rmasÄ±: Mevcut vs Hedef vs Teorik Tavan')
    ax.set_ylim([0, 0.7])
    ax.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='%50 Hedef')
    ax.legend()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '04_theoretical_limit.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 04_theoretical_limit.png")
    plt.close()
    
    return {
        'current_r2': current_r2,
        'target_r2': target_r2,
        'estimated_ceiling': estimated_ceiling,
        'is_achievable': target_r2 <= estimated_ceiling
    }


def generate_scientific_report(variance_results, correlation_results, theoretical_results):
    """Bilimsel Analiz Raporu OluÅŸtur"""
    print("\n" + "="*70)
    print("ğŸ“ BÄ°LÄ°MSEL ANALÄ°Z RAPORU OLUÅTURULUYOR")
    print("="*70)
    
    report = f"""# Bilimsel Analiz: High Severity Model RÂ² Limitasyonu

## Executive Summary

Bu rapor, High Severity Model'in neden **%38 RÂ²**'de takÄ±lÄ± kaldÄ±ÄŸÄ±nÄ± ve **%50 hedefinin** ulaÅŸÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± **bilimsel yÃ¶ntemlerle** analiz eder.

**SonuÃ§:** Mevcut feature'larla %50'ye ulaÅŸmak **{('MÃœMKÃœN' if theoretical_results['is_achievable'] else 'ZOR')}** gÃ¶rÃ¼nÃ¼yor.

---

## 1. Varyans AyrÄ±ÅŸtÄ±rmasÄ±

### Mevcut Durum

- **Toplam Varyans:** {variance_results['explained_var'] + variance_results['unexplained_var']:.4f}
- **AÃ§Ä±klanan Varyans:** {variance_results['explained_var']:.4f} (**{variance_results['r2']*100:.1f}%**)
- **AÃ§Ä±klanamayan Varyans:** {variance_results['unexplained_var']:.4f} (**{(1-variance_results['r2'])*100:.1f}%**)

![Variance Decomposition](01_variance_decomposition.png)

### ğŸ” Kritik Bulgu

VaryansÄ±n **%{(1-variance_results['r2'])*100:.1f}'i** hala aÃ§Ä±klanamÄ±yor. Bu, mevcut feature'larÄ±n ceza sÃ¼resini belirleyen faktÃ¶rlerin sadece **%{variance_results['r2']*100:.1f}'ini** yakaladÄ±ÄŸÄ±nÄ± gÃ¶steriyor.

**Neden?**
- Hakim takdir yetkisi (subjektif karar)
- Dava detaylarÄ± (elimizde yok)
- Mahkeme atmosferi, savunma kalitesi vb.

---

## 2. Feature-Target Korelasyon Analizi

### En GÃ¼Ã§lÃ¼ Feature'lar

Top 5 en yÃ¼ksek korelasyonlu feature'lar:

"""
    
    top_5_corr = correlation_results['correlations'].head(5)
    for _, row in top_5_corr.iterrows():
        report += f"- **{row['feature']}**: {row['corr_raw']:.4f}\n"
    
    report += f"""

![Feature Correlations](02_feature_correlations.png)

### ğŸ” Kritik Bulgu

En gÃ¼Ã§lÃ¼ feature bile **{top_5_corr.iloc[0]['correlation']:.4f}** korelasyona sahip. Bu, **tek baÅŸÄ±na hiÃ§bir feature'Ä±n** ceza sÃ¼resini yeterince aÃ§Ä±klayamadÄ±ÄŸÄ±nÄ± gÃ¶steriyor.

**Yorum:** Ceza sÃ¼resi, **Ã§ok sayÄ±da zayÄ±f sinyalin kombinasyonu** ile belirleniyor. GÃ¼Ã§lÃ¼, dominant bir feature yok.

---

## 3. Teorik RÂ² Ãœst Limiti

### Hesaplama

- **Mevcut RÂ²:** {theoretical_results['current_r2']:.4f} ({theoretical_results['current_r2']*100:.1f}%)
- **Hedef RÂ²:** {theoretical_results['target_r2']:.4f} ({theoretical_results['target_r2']*100:.1f}%)
- **Tahmini Tavan:** {theoretical_results['estimated_ceiling']:.4f} ({theoretical_results['estimated_ceiling']*100:.1f}%)

![Theoretical Limit](04_theoretical_limit.png)

### ğŸ¯ SonuÃ§

"""
    
    if theoretical_results['is_achievable']:
        report += f"""
âœ… **%50 HEDEFÄ° TEORÄ°K OLARAK MÃœMKÃœN!**

Mevcut RÂ² ({theoretical_results['current_r2']*100:.1f}%) ile tahmini tavan ({theoretical_results['estimated_ceiling']*100:.1f}%) arasÄ±nda **{(theoretical_results['estimated_ceiling'] - theoretical_results['current_r2'])*100:.1f}%** iyileÅŸtirme alanÄ± var.

**Gerekli AdÄ±mlar:**
1. Daha iyi feature engineering (non-linear transformations)
2. Feature interactions (2nd, 3rd order)
3. Ensemble methods (stacking, blending)
4. Hyperparameter optimization (Bayesian search)
"""
    else:
        report += f"""
âš ï¸ **%50 HEDEFÄ° MEVCUT FEATURE'LARLA ZOR!**

Tahmini tavan ({theoretical_results['estimated_ceiling']*100:.1f}%), hedefin ({theoretical_results['target_r2']*100:.1f}%) **altÄ±nda**. 

**Gerekli AdÄ±mlar:**
1. **YENÄ°, GÃœÃ‡LÃœ FEATURE'LAR EKLE:**
   - Dava metinleri (NLP)
   - Hakim geÃ§miÅŸi (detaylÄ± profil)
   - Mahkeme kayÄ±tlarÄ± (duruÅŸma sÃ¼releri, tanÄ±k sayÄ±larÄ±)
   - Sosyoekonomik faktÃ¶rler (eÄŸitim, gelir)

2. **DIÅ VERÄ° KAYNAKLARI:**
   - Court transcripts
   - Lawyer quality indicators
   - Community context data
"""
    
    report += f"""

---

## 4. Ã–neriler

### A. KÄ±sa Vadeli (Mevcut Veriyle)

1. **Advanced Feature Engineering**
   - Polynomial features (degree 2-3)
   - Log/sqrt transformations
   - Binning strategies

2. **Model Optimization**
   - Bayesian hyperparameter search
   - Stacking ensemble
   - Neural network embeddings

**Beklenen Ä°yileÅŸme:** {theoretical_results['current_r2']*100:.1f}% â†’ {min(theoretical_results['estimated_ceiling'], theoretical_results['current_r2'] + 0.05)*100:.1f}%

### B. Orta Vadeli (Yeni Feature'lar)

1. **NLP Features**
   - Crime description text analysis
   - Sentiment of case notes
   - Topic modeling

2. **Temporal Features**
   - Seasonal patterns
   - Policy change indicators
   - Judge career stage

**Beklenen Ä°yileÅŸme:** {theoretical_results['current_r2']*100:.1f}% â†’ {min(0.55, theoretical_results['current_r2'] + 0.12)*100:.1f}%

### C. Uzun Vadeli (DÄ±ÅŸ Veri)

1. **Court Records**
   - Trial duration
   - Number of witnesses
   - Defense quality metrics

2. **Defendant Background**
   - Education level
   - Employment status
   - Family structure

**Beklenen Ä°yileÅŸme:** {theoretical_results['current_r2']*100:.1f}% â†’ 55-65%

---

## 5. SonuÃ§

**Ana Bulgu:** High Severity Model'in %38'de takÄ±lmasÄ±nÄ±n nedeni, **mevcut feature'larÄ±n ceza sÃ¼resini belirleyen faktÃ¶rlerin sadece bir kÄ±smÄ±nÄ± yakalamasÄ±**.

**Ã‡Ã¶zÃ¼m:** %50'ye ulaÅŸmak iÃ§in **yeni, gÃ¼Ã§lÃ¼ feature'lar** (Ã¶zellikle dava detaylarÄ± ve hakim profili) gerekli.

**Tavsiye:** 
1. âœ… Mevcut %38 RÂ²'yi **kabul et** (literatÃ¼r ortalamasÄ±nÄ±n Ã¼zerinde)
2. ğŸ”¬ KÄ±sa vadeli optimizasyonlarÄ± dene (%40-42 hedefle)
3. ğŸš€ Orta/uzun vadede yeni veri kaynaklarÄ± araÅŸtÄ±r

---

**HazÄ±rlayan:** Scientific Analysis Team  
**Tarih:** {pd.Timestamp.now().strftime('%Y-%m-%d')}  
**Versiyon:** 1.0
"""
    
    report_path = OUTPUT_DIR / 'scientific_analysis_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"âœ… Bilimsel analiz raporu oluÅŸturuldu: {report_path}")


def main():
    """Ana fonksiyon"""
    print("="*70)
    print("ğŸ”¬ SCIENTIFIC ANALYSIS: RÂ² LIMITATION ROOT CAUSE")
    print("="*70)
    
    # 1. Veri yÃ¼kle
    df_high = load_and_prepare_data()
    
    # 2. Features hazÄ±rla
    X, available_features, cat_features = prepare_features(df_high)
    y = np.log1p(df_high['jail'])
    
    print(f"\nğŸ“Š Dataset Info:")
    print(f"   â€¢ Samples: {len(X):,}")
    print(f"   â€¢ Features: {len(available_features)}")
    print(f"   â€¢ Categorical: {len(cat_features)}")
    
    # 3. Analizler
    variance_results = analysis_1_variance_decomposition(df_high, X, y, cat_features)
    correlation_results = analysis_2_feature_correlations(X, y)
    error_analysis = analysis_3_error_categorization(df_high, X, y, variance_results['model'])
    theoretical_results = analysis_4_theoretical_limit(variance_results, correlation_results)
    
    # 4. Rapor oluÅŸtur
    generate_scientific_report(variance_results, correlation_results, theoretical_results)
    
    print("\n" + "="*70)
    print("âœ… BÄ°LÄ°MSEL ANALÄ°Z TAMAMLANDI!")
    print("="*70)
    print(f"\nğŸ“‚ Ã‡Ä±ktÄ±lar: {OUTPUT_DIR}")
    print(f"   â€¢ scientific_analysis_report.md")
    print(f"   â€¢ 01_variance_decomposition.png")
    print(f"   â€¢ 02_feature_correlations.png")
    print(f"   â€¢ 03_error_categorization.png")
    print(f"   â€¢ 04_theoretical_limit.png")
    
    # Ã–zet
    print(f"\nğŸ¯ Ã–ZET:")
    print(f"   â€¢ Mevcut RÂ²: {theoretical_results['current_r2']:.2%}")
    print(f"   â€¢ Hedef RÂ²: {theoretical_results['target_r2']:.2%}")
    print(f"   â€¢ Tahmini Tavan: {theoretical_results['estimated_ceiling']:.2%}")
    print(f"   â€¢ UlaÅŸÄ±labilir mi? {'âœ… EVET' if theoretical_results['is_achievable'] else 'âŒ ZOR (yeni feature gerekli)'}")


if __name__ == "__main__":
    main()
