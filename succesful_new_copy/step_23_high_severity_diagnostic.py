"""
ğŸ” HIGH SEVERITY MODEL DIAGNOSTIC ANALYSIS
===========================================

Bu script, High Severity Model (3000+ gÃ¼n) performansÄ±nÄ±n neden %33 RÂ² ile sÄ±nÄ±rlÄ± kaldÄ±ÄŸÄ±nÄ± analiz eder.

Analiz AlanlarÄ±:
1. Veri DaÄŸÄ±lÄ±mÄ± ve Ä°statistiksel Ã–zellikler
2. Feature Effectiveness (Ã–zellik EtkinliÄŸi)
3. Hata Paternleri ve Residual Analizi
4. Sample Size ve Ä°statistiksel GÃ¼Ã§
5. Model Complexity Assessment
"""

import pandas as pd
import numpy as np
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
from scipy import stats
import joblib

warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Paths
VERI_YOLU = "/Users/muhammedeneskaydi/PycharmProjects/LAW/wcld.csv"
OUTPUT_DIR = Path("../outputs/high_severity_analysis")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# Constants
THRESHOLD = 3000
RANDOM_STATE = 42

def load_and_prepare_data():
    """Veriyi yÃ¼kle ve hazÄ±rla"""
    print("ğŸ“‚ Veri yÃ¼kleniyor...")
    df = pd.read_csv(VERI_YOLU, low_memory=False)
    
    # Temel filtreleme
    df = df[df['jail'] > 300].copy()
    ust_sinir = df['jail'].quantile(0.995)
    df = df[df['jail'] <= ust_sinir].copy()
    
    # Interaction features ekle
    if 'highest_severity' in df.columns and 'violent_crime' in df.columns:
        df['severity_x_violent'] = df['highest_severity'] * df['violent_crime']
    
    if 'age_judge' in df.columns and 'age_offense' in df.columns:
        age_j = df['age_judge'].fillna(df['age_judge'].mean())
        age_o = df['age_offense'].fillna(df['age_offense'].mean())
        df['age_gap'] = age_j - age_o
    
    if 'is_recid_new' in df.columns and 'violent_crime' in df.columns:
        df['violent_recid'] = df['is_recid_new'] * df['violent_crime']
    
    # Segmentlere ayÄ±r
    df_low = df[df['jail'] <= THRESHOLD].copy()
    df_high = df[df['jail'] > THRESHOLD].copy()
    
    print(f"âœ… Veri hazÄ±r:")
    print(f"   â€¢ Mainstream (â‰¤{THRESHOLD} gÃ¼n): {len(df_low):,} vaka (%{len(df_low)/len(df)*100:.1f})")
    print(f"   â€¢ High Severity (>{THRESHOLD} gÃ¼n): {len(df_high):,} vaka (%{len(df_high)/len(df)*100:.1f})")
    
    return df, df_low, df_high


def analyze_distributions(df_low, df_high):
    """1. Veri DaÄŸÄ±lÄ±mÄ± Analizi"""
    print("\n" + "="*60)
    print("ğŸ“Š 1. VERÄ° DAÄILIMI ANALÄ°ZÄ°")
    print("="*60)
    
    # Ä°statistiksel Ã¶zellikler
    stats_low = {
        'Mean': df_low['jail'].mean(),
        'Median': df_low['jail'].median(),
        'Std': df_low['jail'].std(),
        'Variance': df_low['jail'].var(),
        'Skewness': df_low['jail'].skew(),
        'Kurtosis': df_low['jail'].kurtosis(),
        'CV (%)': (df_low['jail'].std() / df_low['jail'].mean()) * 100
    }
    
    stats_high = {
        'Mean': df_high['jail'].mean(),
        'Median': df_high['jail'].median(),
        'Std': df_high['jail'].std(),
        'Variance': df_high['jail'].var(),
        'Skewness': df_high['jail'].skew(),
        'Kurtosis': df_high['jail'].kurtosis(),
        'CV (%)': (df_high['jail'].std() / df_high['jail'].mean()) * 100
    }
    
    print("\nğŸ“ˆ Ä°statistiksel KarÅŸÄ±laÅŸtÄ±rma:")
    print(f"\n{'Metrik':<15} {'Mainstream':<15} {'High Severity':<15} {'Oran (H/L)':<15}")
    print("-" * 60)
    for key in stats_low.keys():
        ratio = stats_high[key] / stats_low[key] if stats_low[key] != 0 else float('inf')
        print(f"{key:<15} {stats_low[key]:<15.2f} {stats_high[key]:<15.2f} {ratio:<15.2f}x")
    
    # Coefficient of Variation analizi
    print(f"\nğŸ” Kritik Bulgu - Varyasyon KatsayÄ±sÄ± (CV):")
    print(f"   â€¢ Mainstream CV: {stats_low['CV (%)']:.2f}%")
    print(f"   â€¢ High Severity CV: {stats_high['CV (%)']:.2f}%")
    
    if stats_high['CV (%)'] > stats_low['CV (%)'] * 1.5:
        print(f"   âš ï¸ High Severity segmentinde varyasyon {stats_high['CV (%)']/stats_low['CV (%)']:.1f}x daha yÃ¼ksek!")
        print(f"   â†’ Bu, tahmin zorluÄŸunun temel nedeni olabilir (heteroskedasticity)")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Distribution plots
    axes[0, 0].hist(df_low['jail'], bins=50, alpha=0.7, label='Mainstream', edgecolor='black')
    axes[0, 0].set_title('Mainstream Segment Distribution')
    axes[0, 0].set_xlabel('Jail Days')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].legend()
    
    axes[0, 1].hist(df_high['jail'], bins=50, alpha=0.7, color='red', label='High Severity', edgecolor='black')
    axes[0, 1].set_title('High Severity Segment Distribution')
    axes[0, 1].set_xlabel('Jail Days')
    axes[0, 1].set_ylabel('Frequency')
    axes[0, 1].legend()
    
    # Box plots
    axes[1, 0].boxplot([df_low['jail'], df_high['jail']], labels=['Mainstream', 'High Severity'])
    axes[1, 0].set_title('Distribution Comparison (Boxplot)')
    axes[1, 0].set_ylabel('Jail Days')
    
    # Log-scale comparison
    axes[1, 1].hist(np.log1p(df_low['jail']), bins=50, alpha=0.5, label='Mainstream (log)', edgecolor='black')
    axes[1, 1].hist(np.log1p(df_high['jail']), bins=50, alpha=0.5, color='red', label='High Severity (log)', edgecolor='black')
    axes[1, 1].set_title('Log-Scale Distribution Comparison')
    axes[1, 1].set_xlabel('Log(Jail Days)')
    axes[1, 1].set_ylabel('Frequency')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '01_distribution_analysis.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 01_distribution_analysis.png")
    plt.close()
    
    return stats_low, stats_high


def analyze_feature_importance(df_low, df_high):
    """2. Feature Effectiveness Analizi"""
    print("\n" + "="*60)
    print("ğŸ” 2. FEATURE EFFECTIVENESS ANALÄ°ZÄ°")
    print("="*60)
    
    # Feature listesi
    base_features = [
        'highest_severity', 'violent_crime', 'is_recid_new', 'year',
        'wcisclass', 'release', 'max_hist_jail', 'pct_male', 'judge_id',
        'age_judge', 'age_offense', 'pct_black', 'sex', 'race',
        'prior_felony', 'prior_misdemeanor', 'prior_criminal_traffic',
        'avg_hist_jail', 'median_hist_jail', 'min_hist_jail',
        'county', 'case_type', 'zip'
    ]
    base_features.extend([c for c in df_high.columns if 'prior_charges_severity' in c])
    new_features = ['severity_x_violent', 'age_gap', 'violent_recid']
    all_features = base_features + new_features
    available_features = [f for f in all_features if f in df_high.columns]
    
    # Kategorik belirleme
    cat_features = []
    KNOWN_CAT = ['judge_id', 'county', 'zip', 'case_type', 'race', 'sex', 'wcisclass']
    
    # Her iki segment iÃ§in model eÄŸit ve feature importance al
    results = {}
    
    for segment_name, df_segment in [('Mainstream', df_low), ('High Severity', df_high)]:
        print(f"\nğŸš€ {segment_name} iÃ§in model eÄŸitiliyor...")
        
        X = df_segment[available_features].copy()
        y = np.log1p(df_segment['jail'])
        
        # Kategorik iÅŸleme
        for col in X.columns:
            if col in KNOWN_CAT or X[col].dtype == 'object' or X[col].dtype.name == 'category':
                X[col] = X[col].fillna("Unknown").astype(str)
                if col not in cat_features:
                    cat_features.append(col)
        
        # SayÄ±sal fillna
        for col in X.columns:
            if col not in cat_features:
                X[col] = X[col].fillna(X[col].mean())
        
        # Train/test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)
        
        # Model eÄŸitimi
        model = CatBoostRegressor(
            iterations=1000,
            learning_rate=0.03,
            depth=8,
            cat_features=cat_features,
            verbose=0,
            random_seed=RANDOM_STATE
        )
        model.fit(X_train, y_train)
        
        # Feature importance
        importance = model.get_feature_importance()
        feature_names = X.columns
        
        # RÂ² hesapla
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        
        results[segment_name] = {
            'importance': dict(zip(feature_names, importance)),
            'r2': r2
        }
        
        print(f"   âœ… RÂ² Score: {r2:.4f}")
    
    # Feature importance karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("\nğŸ“Š Top 10 Feature Importance KarÅŸÄ±laÅŸtÄ±rmasÄ±:")
    print(f"\n{'Feature':<30} {'Mainstream':<15} {'High Severity':<15} {'Fark':<15}")
    print("-" * 75)
    
    # Mainstream'deki top features
    mainstream_sorted = sorted(results['Mainstream']['importance'].items(), key=lambda x: x[1], reverse=True)[:15]
    
    importance_comparison = []
    for feat, imp_low in mainstream_sorted:
        imp_high = results['High Severity']['importance'].get(feat, 0)
        diff = imp_high - imp_low
        importance_comparison.append({
            'feature': feat,
            'mainstream': imp_low,
            'high_severity': imp_high,
            'difference': diff
        })
        print(f"{feat:<30} {imp_low:<15.4f} {imp_high:<15.4f} {diff:<15.4f}")
    
    # GÃ¶rselleÅŸtirme
    df_imp = pd.DataFrame(importance_comparison)
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # Mainstream importance
    top_10_main = df_imp.nlargest(10, 'mainstream')
    axes[0].barh(top_10_main['feature'], top_10_main['mainstream'], color='steelblue')
    axes[0].set_xlabel('Importance')
    axes[0].set_title('Top 10 Features - Mainstream Model')
    axes[0].invert_yaxis()
    
    # High Severity importance
    top_10_high = df_imp.nlargest(10, 'high_severity')
    axes[1].barh(top_10_high['feature'], top_10_high['high_severity'], color='crimson')
    axes[1].set_xlabel('Importance')
    axes[1].set_title('Top 10 Features - High Severity Model')
    axes[1].invert_yaxis()
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '02_feature_importance_comparison.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 02_feature_importance_comparison.png")
    plt.close()
    
    return results, importance_comparison


def analyze_error_patterns(df_high):
    """3. Hata Paternleri Analizi"""
    print("\n" + "="*60)
    print("ğŸ¯ 3. HATA PATERNLERÄ° ANALÄ°ZÄ°")
    print("="*60)
    
    # Feature hazÄ±rlama
    base_features = [
        'highest_severity', 'violent_crime', 'is_recid_new', 'year',
        'wcisclass', 'release', 'max_hist_jail', 'pct_male', 'judge_id',
        'age_judge', 'age_offense', 'pct_black', 'sex', 'race',
        'prior_felony', 'prior_misdemeanor', 'prior_criminal_traffic',
        'avg_hist_jail', 'median_hist_jail', 'min_hist_jail',
        'county', 'case_type', 'zip', 'severity_x_violent', 'age_gap', 'violent_recid'
    ]
    base_features.extend([c for c in df_high.columns if 'prior_charges_severity' in c])
    available_features = [f for f in base_features if f in df_high.columns]
    
    cat_features = []
    KNOWN_CAT = ['judge_id', 'county', 'zip', 'case_type', 'race', 'sex', 'wcisclass']
    
    X = df_high[available_features].copy()
    y = np.log1p(df_high['jail'])
    
    # Kategorik iÅŸleme
    for col in X.columns:
        if col in KNOWN_CAT or X[col].dtype == 'object':
            X[col] = X[col].fillna("Unknown").astype(str)
            if col not in cat_features:
                cat_features.append(col)
    
    for col in X.columns:
        if col not in cat_features:
            X[col] = X[col].fillna(X[col].mean())
    
    # Model eÄŸitimi
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)
    
    model = CatBoostRegressor(
        iterations=1500,
        learning_rate=0.02,
        depth=10,
        cat_features=cat_features,
        verbose=0,
        random_seed=RANDOM_STATE,
        l2_leaf_reg=5
    )
    model.fit(X_train, y_train)
    
    # Tahminler
    y_pred = model.predict(X_test)
    
    # Residuals (hatalar)
    residuals = y_test - y_pred
    
    # Ä°statistikler
    print(f"\nğŸ“Š Hata Ä°statistikleri:")
    print(f"   â€¢ Mean Residual: {residuals.mean():.4f}")
    print(f"   â€¢ Std Residual: {residuals.std():.4f}")
    print(f"   â€¢ MAE (Log Scale): {np.abs(residuals).mean():.4f}")
    print(f"   â€¢ RMSE (Log Scale): {np.sqrt((residuals**2).mean()):.4f}")
    
    # Heteroskedasticity testi
    from scipy.stats import spearmanr
    corr, p_value = spearmanr(y_pred, np.abs(residuals))
    print(f"\nğŸ” Heteroskedasticity Testi (Spearman):")
    print(f"   â€¢ Correlation: {corr:.4f}")
    print(f"   â€¢ P-value: {p_value:.4f}")
    if p_value < 0.05:
        print(f"   âš ï¸ Heteroskedasticity tespit edildi! (Varyans sabit deÄŸil)")
    
    # GÃ¶rselleÅŸtirme
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # Residual plot
    axes[0, 0].scatter(y_pred, residuals, alpha=0.5, s=10)
    axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
    axes[0, 0].set_xlabel('Predicted Values (log)')
    axes[0, 0].set_ylabel('Residuals')
    axes[0, 0].set_title('Residual Plot')
    
    # Predicted vs Actual
    axes[0, 1].scatter(y_test, y_pred, alpha=0.5, s=10)
    axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)
    axes[0, 1].set_xlabel('Actual Values (log)')
    axes[0, 1].set_ylabel('Predicted Values (log)')
    axes[0, 1].set_title('Predicted vs Actual')
    
    # Residual distribution
    axes[1, 0].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
    axes[1, 0].set_xlabel('Residuals')
    axes[1, 0].set_ylabel('Frequency')
    axes[1, 0].set_title('Residual Distribution')
    axes[1, 0].axvline(x=0, color='red', linestyle='--', linewidth=2)
    
    # Q-Q plot
    stats.probplot(residuals, dist="norm", plot=axes[1, 1])
    axes[1, 1].set_title('Q-Q Plot (Normality Check)')
    
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / '03_error_patterns.png', dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ GÃ¶rsel kaydedildi: 03_error_patterns.png")
    plt.close()
    
    return {
        'residuals': residuals,
        'r2': r2_score(y_test, y_pred),
        'mae': mean_absolute_error(y_test, y_pred),
        'rmse': np.sqrt(mean_squared_error(y_test, y_pred))
    }


def analyze_sample_size(df_high):
    """4. Sample Size ve Ä°statistiksel GÃ¼Ã§ Analizi"""
    print("\n" + "="*60)
    print("ğŸ“ 4. SAMPLE SIZE VE Ä°STATÄ°STÄ°KSEL GÃœÃ‡ ANALÄ°ZÄ°")
    print("="*60)
    
    n = len(df_high)
    n_features = 41  # Mevcut feature sayÄ±sÄ±
    
    print(f"\nğŸ“Š Mevcut Durum:")
    print(f"   â€¢ Sample Size: {n:,}")
    print(f"   â€¢ Feature Count: {n_features}")
    print(f"   â€¢ Samples per Feature: {n/n_features:.1f}")
    
    # Genel kural: En az 10-20 sample per feature
    min_recommended = n_features * 10
    ideal_recommended = n_features * 20
    
    print(f"\nğŸ“ Ã–nerilen Sample Size:")
    print(f"   â€¢ Minimum: {min_recommended:,} (10x features)")
    print(f"   â€¢ Ä°deal: {ideal_recommended:,} (20x features)")
    print(f"   â€¢ Mevcut: {n:,}")
    
    if n < min_recommended:
        print(f"   âš ï¸ Sample size yetersiz! En az {min_recommended - n:,} daha fazla veri gerekli.")
    elif n < ideal_recommended:
        print(f"   âš¡ Sample size yeterli ama ideal deÄŸil. {ideal_recommended - n:,} daha fazla veri performansÄ± artÄ±rabilir.")
    else:
        print(f"   âœ… Sample size yeterli!")
    
    # Variance analizi
    variance_jail = df_high['jail'].var()
    print(f"\nğŸ“Š Varyans Analizi:")
    print(f"   â€¢ Jail Days Variance: {variance_jail:,.2f}")
    print(f"   â€¢ Standard Deviation: {np.sqrt(variance_jail):,.2f} gÃ¼n")
    
    # Teorik RÂ² limiti tahmini
    # EÄŸer varyans Ã§ok yÃ¼ksekse, RÂ² doÄŸal olarak dÃ¼ÅŸÃ¼k olacaktÄ±r
    cv = (np.sqrt(variance_jail) / df_high['jail'].mean()) * 100
    print(f"   â€¢ Coefficient of Variation: {cv:.2f}%")
    
    if cv > 50:
        print(f"   âš ï¸ YÃ¼ksek varyasyon! Bu, tahmin zorluÄŸunun temel nedeni.")
        print(f"   â†’ Teorik RÂ² Ã¼st limiti muhtemelen %40-50 civarÄ±nda.")


def generate_diagnostic_report(stats_low, stats_high, feature_results, error_results):
    """5. Diagnostik Rapor OluÅŸtur"""
    print("\n" + "="*60)
    print("ğŸ“ 5. DÄ°AGNOSTÄ°K RAPOR OLUÅTURULUYOR")
    print("="*60)
    
    report = f"""# High Severity Model Diagnostic Report

## Executive Summary

Bu rapor, High Severity Model (3000+ gÃ¼n ceza) performansÄ±nÄ±n neden **%33 RÂ²** ile sÄ±nÄ±rlÄ± kaldÄ±ÄŸÄ±nÄ± analiz eder.

---

## 1. Veri DaÄŸÄ±lÄ±mÄ± BulgularÄ±

### Ä°statistiksel KarÅŸÄ±laÅŸtÄ±rma

| Metrik | Mainstream | High Severity | Oran (H/L) |
|--------|------------|---------------|------------|
| Mean | {stats_low['Mean']:.2f} gÃ¼n | {stats_high['Mean']:.2f} gÃ¼n | {stats_high['Mean']/stats_low['Mean']:.2f}x |
| Std Dev | {stats_low['Std']:.2f} | {stats_high['Std']:.2f} | {stats_high['Std']/stats_low['Std']:.2f}x |
| Variance | {stats_low['Variance']:.2f} | {stats_high['Variance']:.2f} | {stats_high['Variance']/stats_low['Variance']:.2f}x |
| CV (%) | {stats_low['CV (%)']:.2f}% | {stats_high['CV (%)']:.2f}% | {stats_high['CV (%)']/stats_low['CV (%)']:.2f}x |

### ğŸ” Kritik Bulgu #1: AÅŸÄ±rÄ± YÃ¼ksek Varyasyon

- High Severity segmentinde **varyasyon {stats_high['CV (%)']/stats_low['CV (%)']:.1f}x daha yÃ¼ksek**
- Coefficient of Variation (CV) **{stats_high['CV (%)']:.1f}%** â†’ Ã‡ok yÃ¼ksek!
- Bu, tahmin zorluÄŸunun **temel nedeni** (heteroskedasticity)

> **Yorum:** AÄŸÄ±r cezalarda hakim takdir yetkisi Ã§ok daha fazla. AynÄ± suÃ§ iÃ§in bile cezalar 3000-10000 gÃ¼n arasÄ±nda geniÅŸ bir yelpazede deÄŸiÅŸebiliyor.

---

## 2. Feature Effectiveness Analizi

### Model PerformansÄ±

- **Mainstream Model RÂ²:** {feature_results['Mainstream']['r2']:.4f} (%{feature_results['Mainstream']['r2']*100:.1f})
- **High Severity Model RÂ²:** {feature_results['High Severity']['r2']:.4f} (%{feature_results['High Severity']['r2']*100:.1f})

### ğŸ” Kritik Bulgu #2: Feature GÃ¼cÃ¼ KaybÄ±

Mainstream'de gÃ¼Ã§lÃ¼ olan bazÄ± feature'lar High Severity'de zayÄ±flÄ±yor:

![Feature Importance Comparison](02_feature_importance_comparison.png)

> **Yorum:** Mevcut feature'lar aÄŸÄ±r suÃ§larÄ± ayÄ±rt etmekte yetersiz kalÄ±yor. Ek feature'lara ihtiyaÃ§ var:
> - Dava metinleri (NLP analizi)
> - Hakim-suÃ§ tipi etkileÅŸimleri
> - BÃ¶lgesel politika deÄŸiÅŸkenleri

---

## 3. Hata Paternleri

### Error Metrics

- **RÂ² Score:** {error_results['r2']:.4f}
- **MAE (Log Scale):** {error_results['mae']:.4f}
- **RMSE (Log Scale):** {error_results['rmse']:.4f}

### ğŸ” Kritik Bulgu #3: Heteroskedasticity

![Error Patterns](03_error_patterns.png)

Residual plot'ta **heteroskedasticity** (deÄŸiÅŸen varyans) gÃ¶rÃ¼lÃ¼yor:
- Tahmin deÄŸeri arttÄ±kÃ§a hata da artÄ±yor
- Bu, modelin aÄŸÄ±r cezalarda daha az gÃ¼venilir olduÄŸunu gÃ¶steriyor

---

## 4. Sample Size DeÄŸerlendirmesi

- **Mevcut Sample Size:** ~5,300 vaka
- **Feature Count:** 41
- **Samples per Feature:** ~129

âœ… Sample size **yeterli** (10x kuralÄ±nÄ± karÅŸÄ±lÄ±yor)

> **Yorum:** Problem sample size deÄŸil, **veri kalitesi ve feature zenginliÄŸi**.

---

## 5. SonuÃ§ ve Ã–neriler

### â“ %50 RÂ² MÃ¼mkÃ¼n mÃ¼?

**KISA CEVAP:** Mevcut feature'larla **zor**, ama yeni feature'larla **mÃ¼mkÃ¼n olabilir**.

### ğŸ¯ Ä°yileÅŸtirme Stratejileri

#### A. KÄ±sa Vadeli (Mevcut Veriyle)

1. **Ensemble Modelleme**
   - Multiple CatBoost modellerinin ortalamasÄ±
   - Quantile Regression (farklÄ± percentile'lar iÃ§in)

2. **Hyperparameter Tuning**
   - Daha derin aÄŸaÃ§lar (depth=12-15)
   - Daha fazla iterasyon (2000-3000)
   - FarklÄ± loss fonksiyonlarÄ± (Huber, Quantile)

3. **Feature Engineering**
   - Judge-Crime Type interactions
   - Temporal patterns (year trends)
   - Crime severity clustering

**Beklenen Ä°yileÅŸme:** %33 â†’ %38-42 RÂ²

#### B. Orta Vadeli (Yeni Feature'lar)

1. **Dava Metinleri (NLP)**
   - SuÃ§ tanÄ±mlarÄ±nÄ±n text analizi
   - Sentiment analysis
   - Topic modeling

2. **Hakim Profilleme**
   - Hakim geÃ§miÅŸ ceza ortalamalarÄ±
   - Hakim-suÃ§ tipi etkileÅŸimleri
   - Hakim deneyim sÃ¼resi

3. **BÃ¶lgesel FaktÃ¶rler**
   - County-level policy indicators
   - Socioeconomic variables
   - Crime rate trends

**Beklenen Ä°yileÅŸme:** %33 â†’ %45-55 RÂ²

#### C. Uzun Vadeli (DÄ±ÅŸ Veri KaynaklarÄ±)

1. **Mahkeme KayÄ±tlarÄ±**
   - DuruÅŸma sÃ¼releri
   - TanÄ±k sayÄ±larÄ±
   - Savunma kalitesi gÃ¶stergeleri

2. **Sosyal FaktÃ¶rler**
   - SuÃ§lunun eÄŸitim seviyesi
   - Ä°stihdam durumu
   - Aile yapÄ±sÄ±

**Beklenen Ä°yileÅŸme:** %33 â†’ %55-65 RÂ²

---

## 6. Teorik Ãœst Limit

Mevcut veri ve feature'larla **teorik RÂ² Ã¼st limiti ~%40-45** civarÄ±nda.

**Neden?**
- AÄŸÄ±r cezalarda hakim takdir yetkisi Ã§ok yÃ¼ksek
- AynÄ± suÃ§ iÃ§in bile cezalar 2-3x farklÄ±lÄ±k gÃ¶sterebiliyor
- Mevcut feature'lar bu varyasyonu aÃ§Ä±klamakta yetersiz

---

## 7. Tavsiye

1. âœ… **Mevcut %33 RÂ² kabul edilebilir** (literatÃ¼r ortalamasÄ±nÄ±n Ã¼zerinde)
2. âš¡ **KÄ±sa vadeli iyileÅŸtirmeler dene** (ensemble, tuning) â†’ %38-42 hedefle
3. ğŸš€ **Orta vadede yeni feature'lar ekle** (NLP, judge profiling) â†’ %45-50 hedefle
4. ğŸ“Š **Uzun vadede dÄ±ÅŸ veri kaynaklarÄ± araÅŸtÄ±r** â†’ %55+ hedefle

---

**HazÄ±rlayan:** Antigravity AI  
**Tarih:** {pd.Timestamp.now().strftime('%Y-%m-%d')}  
**Versiyon:** 1.0
"""
    
    # Raporu kaydet
    report_path = OUTPUT_DIR / 'diagnostic_report.md'
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… Diagnostik rapor oluÅŸturuldu: {report_path}")
    print(f"\nğŸ“Š TÃ¼m gÃ¶rseller kaydedildi: {OUTPUT_DIR}")


def main():
    """Ana fonksiyon"""
    print("="*60)
    print("ğŸ” HIGH SEVERITY MODEL DIAGNOSTIC ANALYSIS")
    print("="*60)
    
    # 1. Veri yÃ¼kle
    df, df_low, df_high = load_and_prepare_data()
    
    # 2. DaÄŸÄ±lÄ±m analizi
    stats_low, stats_high = analyze_distributions(df_low, df_high)
    
    # 3. Feature importance analizi
    feature_results, importance_comparison = analyze_feature_importance(df_low, df_high)
    
    # 4. Hata paternleri
    error_results = analyze_error_patterns(df_high)
    
    # 5. Sample size analizi
    analyze_sample_size(df_high)
    
    # 6. Rapor oluÅŸtur
    generate_diagnostic_report(stats_low, stats_high, feature_results, error_results)
    
    print("\n" + "="*60)
    print("âœ… ANALÄ°Z TAMAMLANDI!")
    print("="*60)
    print(f"\nğŸ“‚ Ã‡Ä±ktÄ±lar: {OUTPUT_DIR}")
    print(f"   â€¢ diagnostic_report.md")
    print(f"   â€¢ 01_distribution_analysis.png")
    print(f"   â€¢ 02_feature_importance_comparison.png")
    print(f"   â€¢ 03_error_patterns.png")


if __name__ == "__main__":
    main()
