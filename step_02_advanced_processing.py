
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
import warnings

# Ayarlar
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Yollar
VERI_YOLU = "/Users/muhammedeneskaydi/PycharmProjects/LAW/wcld.csv"
OUTPUT_DIR = Path("outputs/new_analysis_v1")
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

def advanced_processing():
    print(f"ğŸ“‚ Veri yÃ¼kleniyor: {VERI_YOLU}")
    try:
        df = pd.read_csv(VERI_YOLU, low_memory=False)
    except FileNotFoundError:
        print("âŒ HATA: Dosya bulunamadÄ±!")
        return

    # 1. FÄ°LTRELEME (0-300 arasÄ±nÄ± at)
    # KullanÄ±cÄ± 0-300 arasÄ±nÄ± "Ã¶rneklemden Ã§Ä±karacaÄŸÄ±z" dedi. jail > 300 olanlarÄ± alÄ±yoruz.
    if 'jail' in df.columns:
        df_filtered = df[df['jail'] > 300].copy()
        print(f"âœ… FiltrelenmiÅŸ Veri (jail > 300): {df_filtered.shape[0]} satÄ±r")
    else:
        print("âŒ 'jail' kolonu yok!")
        return

    # 2. TUTARSIZLIK ANALÄ°ZÄ° (YÄ±l ve Hakim)
    print("\nğŸ” TutarsÄ±zlÄ±k Analizi BaÅŸlÄ±yor...")
    
    # YÄ±l Analizi
    if 'year' in df_filtered.columns:
        year_stats = df_filtered.groupby('year')['jail'].agg(['mean', 'median', 'count']).sort_index()
        print("\nğŸ“… YÄ±llara GÃ¶re Ceza Ä°statistikleri:")
        print(year_stats)
        
        # YÄ±l bazlÄ± trendi gÃ¶rselleÅŸtir
        plt.figure(figsize=(10, 6))
        sns.lineplot(data=df_filtered, x='year', y='jail', estimator='median', errorbar=None, label='Medyan Ceza')
        sns.lineplot(data=df_filtered, x='year', y='jail', estimator='mean', errorbar=None, label='Ortalama Ceza')
        plt.title('YÄ±llara GÃ¶re Ceza DeÄŸiÅŸimi')
        plt.xlabel('YÄ±l')
        plt.ylabel('Ceza (GÃ¼n)')
        plt.legend()
        plt.savefig(OUTPUT_DIR / "year_trend.png")
        plt.close()
    
    # Hakim Analizi
    if 'judge_id' in df_filtered.columns:
        judge_stats = df_filtered.groupby('judge_id')['jail'].agg(['mean', 'median', 'count', 'std'])
        judge_stats['strictness'] = (judge_stats['mean'] - df_filtered['jail'].mean()) / df_filtered['jail'].std()
        
        print("\nâš–ï¸ Hakim Ä°statistikleri (Ä°lk 10):")
        print(judge_stats.sort_values('count', ascending=False).head(10))
        
        # Hakim tutarsÄ±zlÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtir (En az 50 davasÄ± olanlar)
        active_judges = judge_stats[judge_stats['count'] > 50]
        plt.figure(figsize=(12, 6))
        sns.histplot(active_judges['mean'], kde=True)
        plt.title('Hakimlerin Ortalama Ceza DaÄŸÄ±lÄ±mÄ± (Strictness)')
        plt.xlabel('Ortalama Ceza (GÃ¼n)')
        plt.savefig(OUTPUT_DIR / "judge_distribution.png")
        plt.close()
        
        # Feature Engineering: Hakim sertlik skoru ekle
        # Target encoding mantÄ±ÄŸÄ± (Bunu yaparken data leakage riskine dikkat etmeliyiz ama analiz iÃ§in ÅŸimdilik tÃ¼m veri Ã¼zerinde)
        judge_map = judge_stats['mean'].to_dict()
        df_filtered['judge_mean_jail'] = df_filtered['judge_id'].map(judge_map)
        print("âœ… 'judge_mean_jail' Ã¶zelliÄŸi eklendi (Hakim Tutumu).")

    # 3. VERÄ° Ã–N Ä°ÅLEME VE NORMALÄ°ZASYON
    print("\nâš™ï¸ Veri Ã–n Ä°ÅŸleme ve Normalizasyon...")
    
    # Hedef DeÄŸiÅŸken DÃ¶nÃ¼ÅŸÃ¼mÃ¼: Log Transform (Ã‡Ã¼nkÃ¼ Ã§ok Ã§arpÄ±k)
    df_filtered['jail_log'] = np.log1p(df_filtered['jail'])
    print("âœ… Hedef deÄŸiÅŸken Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapÄ±ldÄ± ('jail_log').")
    
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(df_filtered['jail'], bins=50)
    plt.title('Orijinal Jail DaÄŸÄ±lÄ±mÄ±')
    
    plt.subplot(1, 2, 2)
    sns.histplot(df_filtered['jail_log'], bins=50)
    plt.title('Log Transformed Jail DaÄŸÄ±lÄ±mÄ±')
    plt.savefig(OUTPUT_DIR / "target_transform.png")
    plt.close()

    # Kategorik DeÄŸiÅŸkenleri Belirle
    cat_cols = df_filtered.select_dtypes(include=['object']).columns.tolist()
    num_cols = df_filtered.select_dtypes(include=[np.number]).columns.tolist()
    
    # Eksik verileri doldur (Basit bir strateji: sayÄ±sal -> medyan, kategorik -> mode)
    # Ancak Ã¶nce Ã§ok eksik olanlarÄ± Ã§Ä±karalÄ±m mÄ±?
    # %50'den fazla eksik olanlarÄ± raporlamÄ±ÅŸtÄ±k. 'recid_180d' vs.
    # Åimdilik basit doldurma yapalÄ±m, korelasyonu gÃ¶rmek iÃ§in.
    
    for col in num_cols:
        if df_filtered[col].isnull().sum() > 0:
            df_filtered[col].fillna(df_filtered[col].median(), inplace=True)
            
    for col in cat_cols:
        if df_filtered[col].isnull().sum() > 0:
            df_filtered[col].fillna(df_filtered[col].mode()[0], inplace=True)
            
    # Encoding (Label Encoding for correlation analysis)
    le = LabelEncoder()
    encoded_df = df_filtered.copy()
    for col in cat_cols:
        encoded_df[col] = le.fit_transform(encoded_df[col].astype(str))
        
    # Normalizasyon (MinMax 0-1 aralÄ±ÄŸÄ±)
    scaler = MinMaxScaler()
    # Sadece sayÄ±sal ve encode edilmiÅŸ kolonlar
    cols_to_scale = [c for c in encoded_df.columns if c not in ['jail', 'jail_log', 'new_id', 'case_type_id']] 
    # Hata almamak iÃ§in numeric olanlarÄ± seÃ§
    cols_to_scale = encoded_df.select_dtypes(include=[np.number]).columns.tolist()
    cols_to_scale = [c for c in cols_to_scale if c not in ['jail', 'jail_log']]
    
    if cols_to_scale:
        encoded_df[cols_to_scale] = scaler.fit_transform(encoded_df[cols_to_scale])
        print(f"âœ… Normalizasyon tamamlandÄ± ({len(cols_to_scale)} kolon).")

    # 4. EN ETKÄ°LÄ° PARAMETRELERÄ° BULMA (Korelasyon)
    print("\nğŸ† En Etkili Parametre Analizi (Log Hedef ile)...")
    
    corr_matrix = encoded_df.corr()
    target_corr = corr_matrix['jail_log'].abs().sort_values(ascending=False)
    
    print("\nğŸ” 'jail_log' (DÃ¼zeltilmiÅŸ Ceza) ile En YÃ¼ksek Ä°liÅŸkili 20 Ã–zellik:")
    print(target_corr.head(21)) # jail_log ve jail kendisi dahil
    
    # Feature Importance (Random Forest ile daha saÄŸlam bir seÃ§im)
    from sklearn.ensemble import RandomForestRegressor
    
    # Ã–rneklem al (HÄ±z iÃ§in 10k)
    sample_df = encoded_df.sample(n=min(10000, len(encoded_df)), random_state=42)
    X = sample_df.drop(['jail', 'jail_log'], axis=1)
    # Hata veren kolonlarÄ± (object kalmÄ±ÅŸsa) temizle
    X = X.select_dtypes(include=[np.number])
    y = sample_df['jail_log']
    
    rf = RandomForestRegressor(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1)
    rf.fit(X, y)
    
    importances = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸŒ² Random Forest Feature Importance (En Ã–nemli 20):")
    print(importances.head(20))
    
    importances.to_csv(OUTPUT_DIR / "feature_importance.csv", index=False)
    
    print(f"\nğŸ’¾ TÃ¼m sonuÃ§lar {OUTPUT_DIR} konumuna kaydedildi.")

if __name__ == "__main__":
    advanced_processing()
